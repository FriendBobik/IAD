{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0MMALtdLU6F"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/PyTorch101_ODSC_Europe2020/master/images/linear_dogs.jpg\" width=\"800\">\n",
    "\n",
    "# Основы глубинного обучения: домашнее задание 4\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp2Uf8PGM_zP"
   },
   "source": [
    "**ФИО:** Малиницкий Дмитрий Андреевич\n",
    "\n",
    "**Забавный факт о себе:** у меня 3 ноутбука и 2 3д принтера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2LjTioKM_zP"
   },
   "source": [
    "## Общая информация\n",
    "\n",
    "__Дата выдачи:__ 26.11.2024\n",
    "\n",
    "__Мягкий дедлайн:__ 23:59MSK 10.12.2024\n",
    "\n",
    "__Жесткий дедлайн:__ 23:59MSK 13.12.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGGDD519LWaa"
   },
   "source": [
    "## Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов + 2 бонусных балла.\n",
    "\n",
    "**Дисклеймер:** Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник). Также помните, что простое копирование чужого кода не является самостоятельной работой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "e88B7C_7M_zQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqB7HlFIM_zR",
    "outputId": "7c96af10-5910-4b34-ed0b-72c1df7b5803"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkjGmtWoM_zR"
   },
   "source": [
    "# Описание данных\n",
    "\n",
    "Данные можно [скачать с гугл-диска.](https://drive.google.com/drive/folders/11oCcLplWtp_qm-WuEbfCFP_Mz5K_z3ps?usp=sharing) Если вы делаете задание в колабе, то строчки ниже сами скачают вам данные.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNAzGcglM_zS"
   },
   "source": [
    "В таблице `ria_news.tsv`  лежат данные о новостях, вышедших на сайте РИА-НОВОСТИ с 15 марта 2018 года по 31 декабря 2018 года.\n",
    "\n",
    "- `href` - уникальный идентификатор новости (ссылка на неё)\n",
    "- `date` - дата публикации новости\n",
    "- `time` - время публикации новости\n",
    "- `title` - заголовок новости\n",
    "- `snippet` - краткое описание новости\n",
    "- `text` - текст новости\n",
    "- `category` - категория новости\n",
    "- `keywords` - ключевые слова (подкатегории новости)\n",
    "- `shows` - счётчик с числом просмотров новости на сайте (на момент парсинга)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эххх, жалко что датасет старый, вот бы с 20 года, или с 22, эх мечты мечты..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "U0-iSeRLM_zS",
    "outputId": "8eb3197b-d587-4ee9-f824-b19165342a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201708, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>href</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "      <th>shows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/20181231/1548961410.html</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>31 декабря 2018, 23:52</td>\n",
       "      <td>Нетаньяху не собирается в отставку в случае пр...</td>\n",
       "      <td>Премьер-министр Израиля Биньямин Нетаньяху не ...</td>\n",
       "      <td>МОСКВА, 31 дек - РИА Новости. Премьер-министр ...</td>\n",
       "      <td>В мире</td>\n",
       "      <td>Биньямин Нетаньяху, Израиль, В мире</td>\n",
       "      <td>728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/20181231/1548961364.html</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>31 декабря 2018, 23:19</td>\n",
       "      <td>Макрон в новогоднем обращении затронул тему ре...</td>\n",
       "      <td>Результат реформ не может быть мгновенным, зая...</td>\n",
       "      <td>ПАРИЖ, 31 дек – РИА Новости. Результат реформ ...</td>\n",
       "      <td>В мире</td>\n",
       "      <td>Эммануэль Макрон, Франция, В мире</td>\n",
       "      <td>3086.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/20181231/1548961337.html</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>31 декабря 2018, 23:12</td>\n",
       "      <td>Аарон Рэмзи проведет переговоры с пятью топ-кл...</td>\n",
       "      <td>Полузащитник лондонского \"Арсенала\" Аарон Рэмз...</td>\n",
       "      <td>МОСКВА, 31 дек - РИА Новости. Полузащитник лон...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ФК Арсенал (Лондон)</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/20181231/1548961304.html</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>31 декабря 2018, 23:09</td>\n",
       "      <td>Гол Азмуна принес сборной Ирана победу над кат...</td>\n",
       "      <td>Футболисты сборной Ирана одержали победу над к...</td>\n",
       "      <td>МОСКВА, 31 дек - РИА Новости. Футболисты сборн...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Сердар Азмун, Сборная Ирана по футболу</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/20181231/1548961265.html</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>31 декабря 2018, 23:07</td>\n",
       "      <td>Пятая ракетка мира дель Потро пропустит Открыт...</td>\n",
       "      <td>Аргентинский теннисист Хуан Мартин дель Потро ...</td>\n",
       "      <td>МОСКВА, 31 дек - РИА Новости. Аргентинский тен...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Теннис</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        href        date                    time  \\\n",
       "0  /20181231/1548961410.html  2018-12-31  31 декабря 2018, 23:52   \n",
       "1  /20181231/1548961364.html  2018-12-31  31 декабря 2018, 23:19   \n",
       "2  /20181231/1548961337.html  2018-12-31  31 декабря 2018, 23:12   \n",
       "3  /20181231/1548961304.html  2018-12-31  31 декабря 2018, 23:09   \n",
       "4  /20181231/1548961265.html  2018-12-31  31 декабря 2018, 23:07   \n",
       "\n",
       "                                               title  \\\n",
       "0  Нетаньяху не собирается в отставку в случае пр...   \n",
       "1  Макрон в новогоднем обращении затронул тему ре...   \n",
       "2  Аарон Рэмзи проведет переговоры с пятью топ-кл...   \n",
       "3  Гол Азмуна принес сборной Ирана победу над кат...   \n",
       "4  Пятая ракетка мира дель Потро пропустит Открыт...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  Премьер-министр Израиля Биньямин Нетаньяху не ...   \n",
       "1  Результат реформ не может быть мгновенным, зая...   \n",
       "2  Полузащитник лондонского \"Арсенала\" Аарон Рэмз...   \n",
       "3  Футболисты сборной Ирана одержали победу над к...   \n",
       "4  Аргентинский теннисист Хуан Мартин дель Потро ...   \n",
       "\n",
       "                                                text category  \\\n",
       "0  МОСКВА, 31 дек - РИА Новости. Премьер-министр ...   В мире   \n",
       "1  ПАРИЖ, 31 дек – РИА Новости. Результат реформ ...   В мире   \n",
       "2  МОСКВА, 31 дек - РИА Новости. Полузащитник лон...      NaN   \n",
       "3  МОСКВА, 31 дек - РИА Новости. Футболисты сборн...      NaN   \n",
       "4  МОСКВА, 31 дек - РИА Новости. Аргентинский тен...      NaN   \n",
       "\n",
       "                                     tags   shows  \n",
       "0     Биньямин Нетаньяху, Израиль, В мире   728.0  \n",
       "1       Эммануэль Макрон, Франция, В мире  3086.0  \n",
       "2                     ФК Арсенал (Лондон)   183.0  \n",
       "3  Сердар Азмун, Сборная Ирана по футболу    78.0  \n",
       "4                                  Теннис    79.0  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ria = pd.read_csv('news_data/ria_news.tsv', sep='\\t')\n",
    "df_ria = df_ria[~df_ria.tags.isnull()]\n",
    "print(df_ria.shape)\n",
    "df_ria.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smOrAVujM_zS"
   },
   "source": [
    "Многие новостные агенства поддерживают странички в социальных сетях. Они постят туда самые сочные сюжеты. В таблице `vk_news.tsv` лежат данные о новостях, которые РИА запостили ВКонтакте в период времени с  `2017-09-29 01:28:55` по `2019-02-01 23:13:17`.\n",
    "\n",
    "- `id` - уникальный идентификатор поста\n",
    "- `href` - ссылка на сайт (если она была указана в посте)\n",
    "- `datetime` - дата и время публикации новости\n",
    "- `title` - заголовок новости\n",
    "- `text` - текст новости в социальной сети\n",
    "- `likes` - число лайков под постом\n",
    "- `comments` - число комментариев под постом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "FFZHXKukM_zS",
    "outputId": "7da23095-710b-49c4-fa8c-f73a1294d4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19928, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>href</th>\n",
       "      <th>datetime</th>\n",
       "      <th>title</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24006362</td>\n",
       "      <td>/20190201/1550280358.html</td>\n",
       "      <td>2019-02-01 23:13:17</td>\n",
       "      <td>В ДНР заявили о задержании диверсантов, причас...</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24006240</td>\n",
       "      <td>/20190201/1550268781.html</td>\n",
       "      <td>2019-02-01 22:38:41</td>\n",
       "      <td>Житель Урала \"заминировал\" ТЦ из-за снятия  со...</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24006100</td>\n",
       "      <td>/20190201/1550282212.html</td>\n",
       "      <td>2019-02-01 21:58:52</td>\n",
       "      <td>В Черном море нашли \"потерянный флот Гитлера\"</td>\n",
       "      <td>84</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24005972</td>\n",
       "      <td>/20190202/1550283179.html</td>\n",
       "      <td>2019-02-01 21:27:06</td>\n",
       "      <td>В США освободили задержанную российскую актрис...</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24005764</td>\n",
       "      <td>/20190201/1550262848.html</td>\n",
       "      <td>2019-02-01 20:55:54</td>\n",
       "      <td>Толкнувший Скабееву депутат Рады заявил о гроз...</td>\n",
       "      <td>45</td>\n",
       "      <td>145</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                       href             datetime  \\\n",
       "0  24006362  /20190201/1550280358.html  2019-02-01 23:13:17   \n",
       "1  24006240  /20190201/1550268781.html  2019-02-01 22:38:41   \n",
       "2  24006100  /20190201/1550282212.html  2019-02-01 21:58:52   \n",
       "3  24005972  /20190202/1550283179.html  2019-02-01 21:27:06   \n",
       "4  24005764  /20190201/1550262848.html  2019-02-01 20:55:54   \n",
       "\n",
       "                                               title  likes  comments snippet  \n",
       "0  В ДНР заявили о задержании диверсантов, причас...     15        28     NaN  \n",
       "1  Житель Урала \"заминировал\" ТЦ из-за снятия  со...     32        42     NaN  \n",
       "2      В Черном море нашли \"потерянный флот Гитлера\"     84        23     NaN  \n",
       "3  В США освободили задержанную российскую актрис...     58        35     NaN  \n",
       "4  Толкнувший Скабееву депутат Рады заявил о гроз...     45       145     NaN  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vk = pd.read_csv('news_data/vk_news.tsv', sep='\\t')\n",
    "df_vk['snippet'] = df_vk['text']\n",
    "df_vk.drop('text', axis=1, inplace=True)\n",
    "print(df_vk.shape)\n",
    "df_vk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFaQtcOVM_zT"
   },
   "source": [
    "В таблице `vk_comments.tsv` лежат комментарии к новостям.\n",
    "\n",
    "- `id` - уникальный идентификатор комментария\n",
    "- `post_id` - идентификатор новости, под которой был оставлен комментарий\n",
    "- `datetime` - дата и время, когда был оставлен комментарий\n",
    "- `text` - текст комментария\n",
    "- `likes` - число лайков под комментарием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "HBJvrG6wM_zT",
    "outputId": "c7b02288-fa2d-4e9d-a7bc-803b0404b71f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15604/851848438.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_comments = pd.read_csv('news_data/vk_comments.tsv', sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2612629, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24006366.0</td>\n",
       "      <td>24006362.0</td>\n",
       "      <td>2019-02-01 23:14:14</td>\n",
       "      <td>ЧВК Вагнера?</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24006370.0</td>\n",
       "      <td>24006362.0</td>\n",
       "      <td>2019-02-01 23:15:23</td>\n",
       "      <td>[id4710641|Евгений], выздоравливай.</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24006371.0</td>\n",
       "      <td>24006362.0</td>\n",
       "      <td>2019-02-01 23:16:21</td>\n",
       "      <td>[id442655034|Андрей], искренне желаю этого все...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24006374.0</td>\n",
       "      <td>24006362.0</td>\n",
       "      <td>2019-02-01 23:16:38</td>\n",
       "      <td>Опять про Украину новости?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24006375.0</td>\n",
       "      <td>24006362.0</td>\n",
       "      <td>2019-02-01 23:16:40</td>\n",
       "      <td>Че такое ДНР?</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id     post_id             datetime  \\\n",
       "0  24006366.0  24006362.0  2019-02-01 23:14:14   \n",
       "1  24006370.0  24006362.0  2019-02-01 23:15:23   \n",
       "2  24006371.0  24006362.0  2019-02-01 23:16:21   \n",
       "3  24006374.0  24006362.0  2019-02-01 23:16:38   \n",
       "4  24006375.0  24006362.0  2019-02-01 23:16:40   \n",
       "\n",
       "                                                text  likes  \n",
       "0                                       ЧВК Вагнера?    5.0  \n",
       "1                [id4710641|Евгений], выздоравливай.    3.0  \n",
       "2  [id442655034|Андрей], искренне желаю этого все...    4.0  \n",
       "3                         Опять про Украину новости?    1.0  \n",
       "4                                      Че такое ДНР?    2.0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = pd.read_csv('news_data/vk_comments.tsv', sep='\\t')\n",
    "df_comments = df_comments[~df_comments.text.isnull()]\n",
    "print(df_comments.shape)\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58hpgVLvM_zT"
   },
   "source": [
    "# А что надо сделать то?\n",
    "\n",
    "В тетрадке вам предстоит сделать следующие шаги:\n",
    "\n",
    "1. Обучить нейросеть предсказывать категорию новости\n",
    "2. Построить предсказания для тех новостей, где мы ничего не знаем о категории\n",
    "3. Использовать уже обученный для сентимент-анализа классификатор из библиотеки `hugging face` чтобы предсказать эмоциональную окраску каждого комментария\n",
    "4. Провести аналитику по новостям, а именно построите топы из самых позитивных и негативных категорий и новостей\n",
    "\n",
    "Для первого шага вам будет дан бэйзлайн. Если вы его прогоните, у вас получится базовая модель, которая даст некоторое качество решения задачи. Вам надо будет выяснить, насколько это качество оказалось хорошим, а затем внести в код некоторые улучшения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdb5XYa_M_zT"
   },
   "source": [
    "## Часть 1: категоризация новостей (5 баллов + 1.8 бонусных)\n",
    "\n",
    "Каждой новости в соотвествие поставлены ключевые слова. Будем считать, что эти ключевые слова — тематики новости. Нужно научиться предсказывать тематики по тексту новости. Готовые тематики у нас есть только по новостям с сайта. Они за 2018 год. По новостям из ВКонтакте у нас тематик нет. Мы собираемся их предсказать.\n",
    "\n",
    "Новости, опубликованные ВКонтакте, отличаются от новостей с сайта тем, что у них есть только титул и короткое описание. Странно будет обучать нейросеть на длинных текстах, а потом использовать её на коротких описаниях. Мы не будем так делать. Мы попробуем обучить базовый вариант нейронной сети только на заголовках новостей. Все, кто захочет получить бонусные баллы, смогут попробовать добавить в нейросеть сниппеты (так назыают короткие описания новостей)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmGxSLP-M_zT"
   },
   "source": [
    "## 1.1 Подготовка таргета\n",
    "\n",
    "Поработаем с таргетом. Мы будем предсказывать переменную `tags`. Давайте выясним скоько уникальных тегов существует."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZDxrSvkM_zT",
    "outputId": "f943777c-de05-437d-9cb0-27c24cb8b2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('блог анны завершинской об автоспорте - блоги', 1),\n",
       " ('министерство транспорта рб', 1),\n",
       " ('министерство здравоохранения грузии', 1),\n",
       " ('палех', 1),\n",
       " ('юрий посохов (хореограф)', 1),\n",
       " ('мария александрова', 1),\n",
       " ('том бенсон', 1),\n",
       " ('абдул каюм кочай', 1),\n",
       " ('нуман куртулмуш', 1),\n",
       " ('mipim', 1),\n",
       " ('владимир попов', 1),\n",
       " ('брюно женезио', 1),\n",
       " ('роберт фицо', 1),\n",
       " ('сергей пашинский', 1),\n",
       " ('валерия гонтарева', 1),\n",
       " ('нововоронеж', 1),\n",
       " ('императорское православное палестинское общество', 1),\n",
       " ('event_poslanie_prezidenta_rf_federalnomu_sobraniju', 1),\n",
       " ('фхтр', 1),\n",
       " ('игорь честин', 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# удалим все лишние пробелы и сделаем lowercase\n",
    "df_ria['tags'] = (\n",
    "    df_ria.tags.\n",
    "    apply(lambda w: ','.join([item.strip() for item in  w.lower().split(',')]))\n",
    ")\n",
    "\n",
    "tags = ','.join(list(df_ria.tags.values))\n",
    "tags_cnt = Counter(tags.split(','))\n",
    "\n",
    "print(len(tags_cnt))\n",
    "tags_cnt.most_common()[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Wv68z36M_zT"
   },
   "source": [
    "Всего в выборке есть порядка 13 000 тэгов. Многие встречаются всего по разу. Давайте оставим в выборке только те тэги, которые встречаются более 30 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eA6H34iKM_zU",
    "outputId": "f5b55784-1052-455a-e961-a0fa85bd3932"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tags = {tag for tag,cnt in tags_cnt.most_common() if cnt > 30}\n",
    "len(target_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLrQSyCDM_zU"
   },
   "source": [
    "Закодируем теги для OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "J2fKnLmTM_zU"
   },
   "outputs": [],
   "source": [
    "tag2idx = dict(zip(target_tags, range(len(target_tags))))\n",
    "idx2tag = {jtem: item for item,jtem in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QDNftQqM_zU"
   },
   "source": [
    "Почистим таргет от лишних тэгов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pqA60YiM_zU",
    "outputId": "af3ccfd7-ae21-43fd-ac89-926cc4b36867"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201437, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ria['target_tags'] = (\n",
    "    df_ria.tags.\n",
    "    apply(lambda w: [tag2idx.get(item) for item in  w.split(',') if item in target_tags])\n",
    ")\n",
    "\n",
    "df_ria = df_ria[df_ria.target_tags.apply(len) > 0]\n",
    "df_ria.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0KaYfuQM_zU",
    "outputId": "a8ddc5b6-ffeb-4697-feb8-2a4f011e05b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([438, 1298, 942]), list([984, 1146, 942]), list([1390])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ria.target_tags.values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-hjLFKRM_zU"
   },
   "source": [
    "## 1.2 Подготовка текстов\n",
    "\n",
    "Теперь займёмся предобработкой текстов. Приведём все слова к нижнему регистру и выбросим мусорные символы. В качестве токенов будем рассматривать отдельные слова.\n",
    "\n",
    "Напомню, что мы пока что решили работать только с названиями статей. Поэтому вся предобработка применяется исключительно к ним. **Спойлер:** предобработку для сниппетов вы сделаете сами в первом же задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUEWRZWTQsHy",
    "outputId": "2fe04954-273e-441e-e29d-d689166692a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLHWjtkoM_zU",
    "outputId": "b31d890c-e3bc-4be5-e285-372503938fda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112178"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalise_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # сурово регулярками выкидываем мусорные символы\n",
    "    text = re.sub('[^а-яa-z0-9 ]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "df_ria['title_clean'] = df_ria.title.apply(normalise_text)\n",
    "\n",
    "word_cnt = Counter(word_tokenize(' '.join(df_ria.title_clean.values)))\n",
    "len(word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OGzgIx-M_zV",
    "outputId": "6da55448-8b6f-4370-d3ae-abcc52db7f0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 127323),\n",
       " ('на', 44386),\n",
       " ('с', 26150),\n",
       " ('и', 21771),\n",
       " ('о', 19948),\n",
       " ('по', 17014),\n",
       " ('россии', 13494),\n",
       " ('не', 13483),\n",
       " ('сша', 9942),\n",
       " ('за', 9881)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnt.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ураааа, РОССИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser \n",
    "webbrowser.open(\"https://www.youtube.com/watch?v=xSrsouS0_4U\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ладно, хватит патриотизма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydRq2i-fM_zV"
   },
   "source": [
    "Давайте почистим словарь от стоп-слов и подготовим его к использованию внутри датасета. Мы будем с помощью словаря заменять слова на индексы. Добавим в словарь несколько специальных токенов для неизвестных слов и паддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7W5X3hM2M_zV",
    "outputId": "2c1a7490-7ee6-4829-8b60-2b2a118ab172"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops_ru = set(stopwords.words('russian'))\n",
    "len(stops_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "0h0Eq-fkM_zV"
   },
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    \"#PAD#\": 0, \"#UNK#\": 1\n",
    "}\n",
    "\n",
    "k = 2\n",
    "for word, _ in word_cnt.most_common():\n",
    "    if word not in stops_ru:\n",
    "        vocabulary[word] = k\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xL0VgZT6M_zV",
    "outputId": "a124eeb5-4af3-4eca-c47f-346e96359a09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112030"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9NQcllLM_zV"
   },
   "source": [
    "Завернём код для создания словаря в функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "VAMLLbXxM_zV"
   },
   "outputs": [],
   "source": [
    "def create_vocab(text, stops_ru=stops_ru):\n",
    "\n",
    "    word_cnt = Counter(word_tokenize(text))\n",
    "    vocabulary = {\n",
    "        \"#PAD#\": 0, \"#UNK#\": 1\n",
    "    }\n",
    "\n",
    "    k = 2\n",
    "    for word, _ in word_cnt.most_common():\n",
    "        if word not in stops_ru:\n",
    "            vocabulary[word] = k\n",
    "            k += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnYJT9BXM_zV"
   },
   "source": [
    "__[0.5 балла] Задание 1:__\n",
    "\n",
    "- Cделайте аналогичную предобработку титулов из таблички `df_vk`. Запишите получившийся результат в столбец `title_clean` по аналогии с таблицей `df_ria`.\n",
    "- Сделайте для обеих таблиц предобработку колонок со сниппетами `snippet` и запишите получившийся результат в столбец `snippet_clean`. Все пропуски заполните токеном `\"#UNKN\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "FTlZA9amM_zV"
   },
   "outputs": [],
   "source": [
    "df_vk['title_clean'] = df_vk.title.apply(normalise_text)\n",
    "\n",
    "df_ria['snippet_clean'] = df_ria.snippet.fillna(\"#UNKN\").apply(normalise_text)\n",
    "df_vk['snippet_clean'] = df_vk.snippet.fillna(\"#UNKN\").apply(normalise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  В ДНР заявили о задержании диверсантов, причас...   \n",
      "1  Житель Урала \"заминировал\" ТЦ из-за снятия  со...   \n",
      "2      В Черном море нашли \"потерянный флот Гитлера\"   \n",
      "3  В США освободили задержанную российскую актрис...   \n",
      "4  Толкнувший Скабееву депутат Рады заявил о гроз...   \n",
      "\n",
      "                                         title_clean  \n",
      "0  в днр заявили о задержании диверсантов причаст...  \n",
      "1  житель урала заминировал тц изза снятия  сотов...  \n",
      "2        в черном море нашли потерянный флот гитлера  \n",
      "3  в сша освободили задержанную российскую актрис...  \n",
      "4  толкнувший скабееву депутат рады заявил о гроз...  \n"
     ]
    }
   ],
   "source": [
    "print(df_vk[['title', 'title_clean']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             snippet  \\\n",
      "0  Премьер-министр Израиля Биньямин Нетаньяху не ...   \n",
      "1  Результат реформ не может быть мгновенным, зая...   \n",
      "4  Аргентинский теннисист Хуан Мартин дель Потро ...   \n",
      "5  Суд в американском штате Массачусетс отверг хо...   \n",
      "6  \"Зимняя классика\" - матч регулярного чемпионат...   \n",
      "\n",
      "                                       snippet_clean  \n",
      "0  премьерминистр израиля биньямин нетаньяху не н...  \n",
      "1  результат реформ не может быть мгновенным заяв...  \n",
      "4  аргентинский теннисист хуан мартин дель потро ...  \n",
      "5  суд в американском штате массачусетс отверг хо...  \n",
      "6  зимняя классика  матч регулярного чемпионата н...  \n"
     ]
    }
   ],
   "source": [
    "print(df_ria[['snippet', 'snippet_clean']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              snippet  \\\n",
      "0                                                 NaN   \n",
      "1                                                 NaN   \n",
      "2                                                 NaN   \n",
      "3                                                 NaN   \n",
      "4                                                 NaN   \n",
      "5                                                 NaN   \n",
      "6                                                 NaN   \n",
      "7                                                 NaN   \n",
      "8                                                 NaN   \n",
      "9                                                 NaN   \n",
      "10                                                NaN   \n",
      "11                                                NaN   \n",
      "12                                                NaN   \n",
      "13                                                NaN   \n",
      "14                                                NaN   \n",
      "15                                                NaN   \n",
      "16  Ранее в соцсетях появились кадры, на которых з...   \n",
      "17                                                NaN   \n",
      "18                                                NaN   \n",
      "19                                                NaN   \n",
      "\n",
      "                                        snippet_clean  \n",
      "0                                                unkn  \n",
      "1                                                unkn  \n",
      "2                                                unkn  \n",
      "3                                                unkn  \n",
      "4                                                unkn  \n",
      "5                                                unkn  \n",
      "6                                                unkn  \n",
      "7                                                unkn  \n",
      "8                                                unkn  \n",
      "9                                                unkn  \n",
      "10                                               unkn  \n",
      "11                                               unkn  \n",
      "12                                               unkn  \n",
      "13                                               unkn  \n",
      "14                                               unkn  \n",
      "15                                               unkn  \n",
      "16  ранее в соцсетях появились кадры на которых за...  \n",
      "17                                               unkn  \n",
      "18                                               unkn  \n",
      "19                                               unkn  \n"
     ]
    }
   ],
   "source": [
    "print(df_vk[['snippet', 'snippet_clean']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ладно всё окей, а тоя испугался\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrRKg_QHM_zV"
   },
   "source": [
    "## 1.3 Поставка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2vPPWsEM_zV"
   },
   "source": [
    "Пересечение сайта и ВК по опубликованным новостям довольно маленькое. Мы обучаем модель на данных с сайта. Предсказания мы будем строить на данных из ВК. У этих данных разная природа. В ВК описание статей и заголовки короче. Модель может хорошо показать себя на данных с новостного сайта, но сильно просесть в качестве на данных из ВК.\n",
    "\n",
    "Давайте сохраним пересечение в отдельную табличку, чтобы на нём можно было понять, насколько сильно деградирует модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KjCNaTLM_zW",
    "outputId": "a0de14af-5d32-4dff-89f7-3b04a7e35271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер отложенной выборки: 1128\n"
     ]
    }
   ],
   "source": [
    "ria_hrefs = set(df_ria.href.values)\n",
    "vk_hrefs = set(df_vk.href.values)\n",
    "test_hrefs = (vk_hrefs & ria_hrefs)\n",
    "\n",
    "print('Размер отложенной выборки:', len(test_hrefs))\n",
    "\n",
    "df = df_ria[~df_ria.href.isin(test_hrefs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVutzHbPM_zg"
   },
   "source": [
    "По странному совпадению (я правда не знаю почему) пересечение лежит в декабре. Мы будем его использовать как тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fGs6y6PM_zg",
    "outputId": "5a639a55-e194-4d78-bba0-f6297339f49d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018-12-06', '2018-12-31')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ria[df_ria.href.isin(test_hrefs)].date.min(), df_ria[df_ria.href.isin(test_hrefs)].date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ_7VGvnM_zg"
   },
   "source": [
    "Предположим, что мы делим выборку на обучающую и тестовую случайно. За один и тот же промежуток времени может выйти довольно большое число новостей с одинаковым заголовком. Давайте представим себе, что в тесте и трэйне есть много статей про одно и то же событие. Модель научилась на обучающей выборке хорошо его тегировать. Остальные события модель тегирует намного хуже. Метрики на тестовой выборке высокие. В следующем месяце СМИ перестают освещать это событие, в потоке новостей совершенно другие новости. Качество модели резко проседает.\n",
    "\n",
    "Чтобы не напороться на завышенные метрики, обычно выборку дробят на обучающую и тестовую по времени. Тогда статьи из теста будут имитировать поток новых новостей, освещающих новые события."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WscwQXAuM_zg",
    "outputId": "0a83dd25-a157-45e9-b757-f9fdf70a56e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018-03-15', '2018-12-31')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.min(), df.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzM_6-ntM_zg"
   },
   "source": [
    "__[0.5 балла] Задание 2:__ Разбейте выборку на обучающую, валидационную и тестовую. В тест возьмите весь декабрь. В валидацию октябрь и ноябрь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "Yzr5nNIvM_zg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15604/3942277652.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'] = pd.to_datetime(df['date'])\n"
     ]
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df_test = df[df['date'].dt.month == 12]\n",
    "df_val = df[df['date'].dt.month.isin([10, 11])]\n",
    "df_train = df[~df['date'].dt.month.isin([10, 11, 12])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (136875, 12)\n",
      "Validation set: (43258, 12)\n",
      "Test set: (20176, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set: {df_train.shape}\")\n",
    "print(f\"Validation set: {df_val.shape}\")\n",
    "print(f\"Test set: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlG-87v8M_zh"
   },
   "source": [
    "Сформируем отложенную выборку (пересечение ВКонтакте и РИА)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "KmBZjIx4M_zh",
    "outputId": "6b4bc36b-9d88-4182-a0b0-9d4c4314a248"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>href</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>target_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/20181231/1548960744.html</td>\n",
       "      <td>митрополит рязанский раскритиковал иронию судьбы</td>\n",
       "      <td>[1115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/20181231/1548958617.html</td>\n",
       "      <td>на украине позавидовали стене на границе с крымом</td>\n",
       "      <td>[1359, 942]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/20181231/1548957394.html</td>\n",
       "      <td>в госдуме предложили отказаться от газа в жилы...</td>\n",
       "      <td>[1556, 1556, 1201, 194, 973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/20181231/1548954909.html</td>\n",
       "      <td>названы лучшие средства от похмелья</td>\n",
       "      <td>[1115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/20181231/1548957120.html</td>\n",
       "      <td>новогоднее поздравление порошенко разозлило по...</td>\n",
       "      <td>[1359, 942]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        href  \\\n",
       "0  /20181231/1548960744.html   \n",
       "1  /20181231/1548958617.html   \n",
       "2  /20181231/1548957394.html   \n",
       "3  /20181231/1548954909.html   \n",
       "4  /20181231/1548957120.html   \n",
       "\n",
       "                                         title_clean  \\\n",
       "0   митрополит рязанский раскритиковал иронию судьбы   \n",
       "1  на украине позавидовали стене на границе с крымом   \n",
       "2  в госдуме предложили отказаться от газа в жилы...   \n",
       "3                названы лучшие средства от похмелья   \n",
       "4  новогоднее поздравление порошенко разозлило по...   \n",
       "\n",
       "                    target_tags  \n",
       "0                        [1115]  \n",
       "1                   [1359, 942]  \n",
       "2  [1556, 1556, 1201, 194, 973]  \n",
       "3                        [1115]  \n",
       "4                   [1359, 942]  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oob = df_vk[df_vk.href.isin(test_hrefs)][['href', 'title_clean']]\n",
    "\n",
    "df_ria_oob = df_ria[df_ria.href.isin(test_hrefs)][['href', 'target_tags']]\n",
    "df_oob = df_oob.set_index('href').join(df_ria_oob.set_index('href')).reset_index()\n",
    "df_oob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laZ8tkFjM_zh"
   },
   "source": [
    "Напишем датасет для поставки данных в нейросеть.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "0if_RkA_M_zh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, target, title, vocab, vocab_size, max_title_len, max_classes, snippet=None, max_snippet_len=None):\n",
    "\n",
    "       self.vocab = {word: idx  for word,idx in vocab.items() if idx < vocab_size}\n",
    "       self.max_classes = max_classes\n",
    "       self.y=self.target_ohe(target)\n",
    "       self.X_title = self.create_text(title, max_title_len)\n",
    "\n",
    "    def target_ohe(self, target):\n",
    "        y = torch.zeros((len(target), self.max_classes))\n",
    "        for i, t in enumerate(target):\n",
    "            y[[i]*len(t), t] = 1.0\n",
    "        return y\n",
    "\n",
    "    def create_text(self, texts, max_len):\n",
    "        result = [ ]\n",
    "        for sent in texts:\n",
    "            # {#PAD: 0, #UNKN: 1}\n",
    "            sent_tokenize = [self.vocab.get(item, 1) for item in word_tokenize(sent)]\n",
    "\n",
    "            # приводим все тексты к max_len\n",
    "            if len(sent_tokenize) >= max_len:\n",
    "                sent_tokenize = sent_tokenize[:max_len]\n",
    "            else:\n",
    "                sent_tokenize += [0] * (max_len - len(sent_tokenize))\n",
    "            result.append(sent_tokenize)\n",
    "        return torch.tensor(result, dtype=torch.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X_title[idx, :], self.y[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQMfQ1ZkM_zh"
   },
   "source": [
    "__[0.5 балла] Задание 3:__ Сейчас датасет умеет работать только с полем `title_clean`. Давайте сделаем этот датасет более многофукнциональным и добавим в него возможность добавить в обработку данных сниппет.\n",
    "\n",
    "1. Внутри датасета `snippet` надо обработать точно также как и `title`.\n",
    "2. Если `snippet=None`, датасет должен вернуть два объекта: `X_title, y`. В обратном случае датасет должен вернуть три объекта.\n",
    "\n",
    "**Важно:** Весь код ниже работает сейчас без сниппета. Он не должен развалиться от того, что сниппет в нём нигде не указан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "1ekPnlTlM_zh"
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, target, title, vocab, vocab_size, max_title_len, max_classes, \n",
    "                 snippet=None, max_snippet_len=None):\n",
    "        self.vocab = {word: idx for word, idx in vocab.items() if idx < vocab_size}\n",
    "        self.max_classes = max_classes\n",
    "        self.y = self.target_ohe(target)\n",
    "        self.X_title = self.create_text(title, max_title_len)\n",
    "        \n",
    "        if snippet is not None and max_snippet_len is not None:\n",
    "            self.has_snippet = True\n",
    "            self.X_snippet = self.create_text(snippet, max_snippet_len)\n",
    "        else:\n",
    "            self.has_snippet = False\n",
    "\n",
    "    def target_ohe(self, target):\n",
    "        y = torch.zeros((len(target), self.max_classes))\n",
    "        for i, t in enumerate(target):\n",
    "            y[[i]*len(t), t] = 1.0\n",
    "        return y\n",
    "\n",
    "    def create_text(self, texts, max_len):\n",
    "        result = []\n",
    "        for sent in texts:\n",
    "            sent_tokenize = [self.vocab.get(item, 1) for item in word_tokenize(sent)]\n",
    "            if len(sent_tokenize) >= max_len:\n",
    "                sent_tokenize = sent_tokenize[:max_len]\n",
    "            else:\n",
    "                sent_tokenize += [0] * (max_len - len(sent_tokenize))\n",
    "            result.append(sent_tokenize)\n",
    "        return torch.tensor(result, dtype=torch.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_snippet:\n",
    "            return (self.X_title[idx, :], self.X_snippet[idx, :], self.y[idx])\n",
    "        else:\n",
    "            return (self.X_title[idx, :], self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v_Rgi_DM_zh"
   },
   "source": [
    "Объявим датасеты, оставим в словаре 30 000 самых частотных слов. Будем смотреть на титулы максимальной длины 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "2zjgEiWWM_zh"
   },
   "outputs": [],
   "source": [
    "CLASSES_NUM = len(idx2tag)\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_TITLE_LEN = 20\n",
    "\n",
    "# словарь создаем по всей выборке\n",
    "vocabulary = create_vocab(' '.join(df_ria.title_clean.values))\n",
    "\n",
    "# объявляем датасеты\n",
    "train_dataset = NewsDataset(df_train.target_tags.values, df_train.title_clean.values, vocabulary, VOCAB_SIZE, MAX_TITLE_LEN, CLASSES_NUM )\n",
    "val_dataset = NewsDataset(df_val.target_tags.values, df_val.title_clean.values, vocabulary, VOCAB_SIZE, MAX_TITLE_LEN, CLASSES_NUM )\n",
    "test_dataset = NewsDataset(df_test.target_tags.values, df_test.title_clean.values, vocabulary, VOCAB_SIZE, MAX_TITLE_LEN, CLASSES_NUM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "kNdo56v0M_zh"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=4096, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvQXMtISM_zh"
   },
   "source": [
    "## 1.4 Архитектуры\n",
    "\n",
    "Соберём базовую архитектуру для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "dN8DAIIkM_zh"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, title):\n",
    "        embedded = self.embedding(title)\n",
    "        embedded = embedded.mean(dim=1)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slQEltBQM_zh"
   },
   "source": [
    "Соберём в `pytorch_lightning` модуль для обучения нейронки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "5E6J3of9M_zi"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class TrainLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate, criterion):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, title):\n",
    "        result = self.model(title)\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        title, target = train_batch\n",
    "        logits = self.model(title)\n",
    "        loss = self.criterion(logits, target)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, prog_bar=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        title, target = val_batch\n",
    "        logits = self.model(title)\n",
    "        loss = self.criterion(logits, target)\n",
    "        self.log(\n",
    "            \"val_loss\", loss, prog_bar=True\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BirneLfM_zi"
   },
   "source": [
    "Обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "tensor([[0.2666, 0.6274, 0.2696],\n",
      "        [0.4414, 0.2969, 0.8317],\n",
      "        [0.1053, 0.2695, 0.3588]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "x = torch.rand(3, 3).to(device)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349,
     "referenced_widgets": [
      "3ef009aeabae43b48bc913a592d1a019",
      "75b99a050a37493389ff96724a9a3b72",
      "78fe9e3a3d614de9a6f3efacde67d247",
      "2d71626a24cb45f39095e7e3fd621707",
      "4f75594be29e49cc99fe61a6787e7564",
      "9ab912602cc24346a011fdf58feafbde",
      "af211f3be2ce4fb1b6733fa552ed041b",
      "84f32a71a7cd4de6b55a9558717d00ef",
      "5db675de6b8348e9a9823384541bdfff",
      "6fa8591a1a834d8785b1ebe8eff2b10d",
      "acfec13aec454320b585c0b8331e9448",
      "1ae6951f8b984b318210df9d009479d8",
      "d9456651273140f9bd6e587ab852eeec",
      "b174cb3407a04b2989e6049555265446",
      "7bd174d7cd2346be9db9d5a0a170f8cb",
      "3ada921ee7644de997b57ffd2c57044d",
      "17e62da8657f44d9a998f4acbbd29671",
      "be93fd003ef54d7e85bb1fcaa894e467",
      "938d3da0eef54ee997fc0ce7ccf1807b",
      "1c10405770654b90a3635ba4acabb6ef",
      "2da5b84f9384445a8d9db58ef3dafa5e",
      "abcb57f0947f413d9cb83c62c6529562",
      "d8cd82c7af8747148b5b4750e8f1372a",
      "17d8af2b171440cba00f8b2df1a08073",
      "fde8213392c546b58ad8a81aacf4fcb3",
      "09093bb710bd4903abf2e10db6efc0a0",
      "d92ec28e7e6c47c4b2e3669b502b8b9e",
      "201a41be363549eaa0fdc903fb41f83e",
      "60c4aa6c4c8145949cd16ec0670227a1",
      "58fbdeafd7ee40cd9392cd4a62c49b81",
      "286fbf1303dc4f0b9c1bb3cfb34f920e",
      "8fc5cc9d0df74d5081fa3f8c89595daf",
      "0cbe638b1d354bea83ccba54ee3d1f5c",
      "f2b6c28c36e64f66b54834f543cec282",
      "2d98065fccb34cb1b44740aa4510f0e7",
      "2aa0bc933aae46729672d55e60c17616",
      "950950a569914ad1a263f3f295e57852",
      "2f4dcf310c634bdba125e8bf46d3db26",
      "6072601881c341fb8e5329fab05ee52d",
      "13a1fd50c8f24f12a0f28485b6168606",
      "546d1deb5c934916b722fb9ab0ce5e67",
      "86ed101f86384b9b92eb3faccae5bca2",
      "cba28fcac7c349e8ac9794d507768744",
      "7bb5cc7b77654cc0b27a22552941a1ce",
      "b6179f72e8f94772a4e1e2a503bc1d47",
      "9ec11067fc81414d97457b47e93b78a1",
      "6351ca36d8d648afa472093df7e73e45",
      "d1efd4fe9adc4243a81d401784b77530",
      "2cb3f56ff669424da4e9bace352831b8",
      "3dc20b49832e46c6ad70dc754f43555f",
      "100219bd594645c5b60c3da4a44344fe",
      "7a6a637bb7664b4495b2b56b45d88ee4",
      "74bf3b2a873e40ffa139f1afe652ffe8",
      "640ac5a06820498585c48a1b69355d0d",
      "515318593400421487560182023801f1",
      "2a38ec8f52d74e018a4c1876d3a2f874",
      "fd22c38d837a404ab892ddaddf4a8988",
      "a474d8d377674579b3c9f5327fbb1862",
      "0029cbbc48c64effb57bffff5d199e34",
      "e9d2f22400db472283508e8c01f08fbb",
      "46204f92abf1414da360d79b4ca7a924",
      "0a54156fabcd4ef3b73b038b71485a6a",
      "2e02be5bf0be40bf87f1364b1fd587ec",
      "560450b99c8f4636b3a700db3fc21253",
      "0140e6c8951147b6a7e079682e938e7f",
      "534e121744814f4d8880072004513e37",
      "b23fac677f5e4e8884f84fe60585330d",
      "1ae57433e573436b8bbdf6c22630930d",
      "6769dc278aae487db30d78caa97c6dd8",
      "b5b6d56829af44d99b904176b9552101",
      "7dc052c589534aa986569d863596a2a3",
      "5d422f8cbd47402bb8f28b258e5abed0",
      "80d177b53a5f4fe0a733d3b7d36e9b30",
      "71c435b7fa6f4a0fb44a4c8a8b5a73f6",
      "c05c8b6064f94652a1f2ed5c3bb7c2b0",
      "c44756a0ec83400f9f6daf8d1179ad91",
      "a2903f89f4584f16b9f5b412d2730672"
     ]
    },
    "id": "5ShTLAStM_zi",
    "outputId": "c1894412-c82d-4061-971d-4c732c3ae599"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | SimpleClassifier | 3.5 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "13.906    Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2139/2139 [00:14<00:00, 147.58it/s, v_num=18, train_loss=12.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=18, train_loss=12.20, val_loss=13.80]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2139/2139 [00:15<00:00, 135.53it/s, v_num=18, train_loss=12.90, val_loss=13.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=18, train_loss=12.90, val_loss=12.10]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2139/2139 [00:15<00:00, 133.72it/s, v_num=18, train_loss=8.990, val_loss=12.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=18, train_loss=8.990, val_loss=11.30]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2139/2139 [00:15<00:00, 136.83it/s, v_num=18, train_loss=8.600, val_loss=11.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=18, train_loss=8.600, val_loss=10.90]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2139/2139 [00:15<00:00, 137.37it/s, v_num=18, train_loss=7.090, val_loss=10.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2139/2139 [00:17<00:00, 120.34it/s, v_num=18, train_loss=7.090, val_loss=10.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2139/2139 [00:17<00:00, 119.36it/s, v_num=18, train_loss=7.090, val_loss=10.70]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "\n",
    "model_baseline = SimpleClassifier(VOCAB_SIZE, EMBEDDING_DIM, CLASSES_NUM)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train_module = TrainLightningModule(model_baseline, LR, criterion)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=EPOCHS)\n",
    "trainer.fit(train_module, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj9QJ-xEM_zi"
   },
   "source": [
    "__[1 балл] Задание 4:__  Модель выше работает только с титулом.\n",
    "\n",
    "- Залоггируйте её обучение на [WandB](https://wandb.ai/).\n",
    "- Соберите архитектуру, которая будет принимать на вход не только титул, но ещё и сниппет. В этой архитектуре должно происходить следующее:\n",
    "\n",
    "1. Общий слой `nn.Embedding` применяется к сниппету и титулу параллельно.\n",
    "2. Происходит усреднее по текстам.\n",
    "3. Вектора конкатятся в один длины 600\n",
    "4. Линейный слой делает классификацию\n",
    "\n",
    "Обучите эту модель. Сравните траектории обучения на WandB. Прикрепите ссылку на дашборд либо скришот к тетрадке.\n",
    "\n",
    "Даталоадеры придётся объявить заново с учётом сниппетов. Правда ли, что она бьёт на валидационной выборке модель, обученную только на титулах статей?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "pOJSMhYMM_zi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb_key import API_KEY \n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb.login(key=API_KEY)\n",
    "wandb_logger = WandbLogger(project=\"news-categorization\", log_model=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SNIPPET_LEN = 30\n",
    "train_dataset_snippet = NewsDataset(\n",
    "    target=df_train.target_tags.values, \n",
    "    title=df_train.title_clean.values, \n",
    "    vocab=vocabulary, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    max_title_len=MAX_TITLE_LEN, \n",
    "    max_classes=CLASSES_NUM,\n",
    "    snippet=df_train.snippet_clean.values,\n",
    "    max_snippet_len=MAX_SNIPPET_LEN\n",
    ")\n",
    "\n",
    "val_dataset_snippet = NewsDataset(\n",
    "    target=df_val.target_tags.values, \n",
    "    title=df_val.title_clean.values, \n",
    "    vocab=vocabulary, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    max_title_len=MAX_TITLE_LEN, \n",
    "    max_classes=CLASSES_NUM,\n",
    "    snippet=df_val.snippet_clean.values,\n",
    "    max_snippet_len=MAX_SNIPPET_LEN\n",
    ")\n",
    "\n",
    "test_dataset_snippet = NewsDataset(\n",
    "    target=df_test.target_tags.values, \n",
    "    title=df_test.title_clean.values, \n",
    "    vocab=vocabulary, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    max_title_len=MAX_TITLE_LEN, \n",
    "    max_classes=CLASSES_NUM,\n",
    "    snippet=df_test.snippet_clean.values,\n",
    "    max_snippet_len=MAX_SNIPPET_LEN\n",
    ")\n",
    "\n",
    "train_dataloader_snippet = DataLoader(train_dataset_snippet, shuffle=True, batch_size=64, num_workers=4)\n",
    "val_dataloader_snippet = DataLoader(val_dataset_snippet, shuffle=False, batch_size=4096, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SnippetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(SnippetClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, title, snippet):\n",
    "        embedded_title = self.embedding(title)\n",
    "        embedded_title = embedded_title.mean(dim=1)\n",
    "        \n",
    "        embedded_snippet = self.embedding(snippet)\n",
    "        embedded_snippet = embedded_snippet.mean(dim=1)\n",
    "        \n",
    "        combined = torch.cat((embedded_title, embedded_snippet), dim=1)\n",
    "        \n",
    "        out = self.fc(combined)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnippetLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate, criterion):\n",
    "        super(SnippetLightningModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, title, snippet):\n",
    "        return self.model(title, snippet)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        title, snippet, target = batch\n",
    "        logits = self.model(title, snippet)\n",
    "        loss = self.criterion(logits, target)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        title, snippet, target = batch\n",
    "        logits = self.model(title, snippet)\n",
    "        loss = self.criterion(logits, target)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/myenv/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/user/myenv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory ./news-categorization/14y6jq8z/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | SnippetClassifier | 4.0 M  | train\n",
      "1 | criterion | CrossEntropyLoss  | 0      | train\n",
      "--------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "15.806    Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2139/2139 [00:15<00:00, 135.14it/s, v_num=jq8z, train_loss=11.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=jq8z, train_loss=11.10, val_loss=13.70]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2139/2139 [00:17<00:00, 122.82it/s, v_num=jq8z, train_loss=6.180, val_loss=13.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=jq8z, train_loss=6.180, val_loss=11.60]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2139/2139 [00:17<00:00, 124.20it/s, v_num=jq8z, train_loss=7.250, val_loss=11.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=jq8z, train_loss=7.250, val_loss=10.80]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2139/2139 [00:17<00:00, 122.65it/s, v_num=jq8z, train_loss=7.260, val_loss=10.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/2139 [00:00<?, ?it/s, v_num=jq8z, train_loss=7.260, val_loss=10.50]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2139/2139 [00:17<00:00, 122.92it/s, v_num=jq8z, train_loss=5.400, val_loss=10.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2139/2139 [00:19<00:00, 108.05it/s, v_num=jq8z, train_loss=5.400, val_loss=10.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2139/2139 [00:20<00:00, 105.17it/s, v_num=jq8z, train_loss=5.400, val_loss=10.40]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "\n",
    "model_snippet = SnippetClassifier(VOCAB_SIZE, EMBEDDING_DIM, CLASSES_NUM)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "snippet_module = SnippetLightningModule(model_snippet, LR, criterion)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    max_epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.fit(snippet_module, train_dataloader_snippet, val_dataloader_snippet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKdBptFbM_zi"
   },
   "source": [
    "## 1.5 Инференс и оценка качества моделей\n",
    "\n",
    "Для каждой новости нам надо предсказывать несколько тэгов. То есть в нашем случае настоящее значение таргета это множество из тэгов $y_i = [tag1, tag2, tag3]$. Прогноз модели также множество из тэгов $\\hat y_i = [tag1, tag4]$.\n",
    "\n",
    "Будем считать метрики качества следующим образом (под $|A|$ имеется в виду мощность множества, то есть число элементов в нём):\n",
    "\n",
    "$$\n",
    "Precision = \\frac{1}{n} \\sum_{i = 1}^n \\frac{|y_i \\cap \\hat{y}_i|}{|\\hat{y}_i|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Recall = \\frac{1}{n} \\sum_{i = 1}^n \\frac{|y_i \\cap \\hat{y}_i|}{|y_i|}\n",
    "$$\n",
    "\n",
    "Также можно считать аналог Accuracy, но это не самая удачная идея, так как у нас в выборке огромное число нулей и эта метрика при любом разумном пороге для принятия решения будет очень высокой.\n",
    "\n",
    "$$\n",
    "Exact Match = \\frac{1}{n} \\cdot \\frac{1}{k} \\sum_{i = 1}^n \\sum_{k=1}^K [y_{ij} = \\hat{y}_{ij}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "tgBsB48FM_zi"
   },
   "outputs": [],
   "source": [
    "def precision(target, y_pred):\n",
    "    num = ((y_pred == 1) & (target == 1)).sum(dim=1)\n",
    "    denum = (y_pred == 1).sum(dim=1)\n",
    "    return (num/(denum + 1e-5)).mean().item()\n",
    "\n",
    "def recall(target, y_pred):\n",
    "    num = ((y_pred == 1) & (target == 1)).sum(dim=1)\n",
    "    denum = (target == 1).sum(dim=1)\n",
    "    return (num/(denum + 1e-5)).mean().item()\n",
    "\n",
    "def exact_match(target, y_pred):\n",
    "    return (1.*(y_pred == target)).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD9UeFECM_zi"
   },
   "source": [
    "Построим прогноз на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "JHHDcGUDM_zi"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=test_dataset.__len__())\n",
    "\n",
    "for title, target in test_dataloader:\n",
    "    logit = model_baseline(title)\n",
    "    pred_prob = F.softmax(logit, dim=1)\n",
    "\n",
    "assert pred_prob.shape[0] == test_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzbiK9llM_zi"
   },
   "source": [
    "Теперь выбирая различное значение порога, мы можем получать разные предсказания. Если взять очень большое значение порога, то метрики сильно просядут, так как во многих документах никакого прогноза не будет построено вообще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2UI7GKqM_zj",
    "outputId": "c0385b7a-87c8-4e0a-914c-9ac9a93ce67a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.9954836964607239\n",
      "Precision: 0.2893848717212677\n",
      "Recall: 0.7641034126281738\n"
     ]
    }
   ],
   "source": [
    "TRESHOLD = 0.01\n",
    "y_pred = 1*(pred_prob > TRESHOLD)\n",
    "\n",
    "print('Exact Match:', exact_match(target, y_pred))\n",
    "print('Precision:', precision(target, y_pred))\n",
    "print('Recall:', recall(target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6v3CTYheM_zj",
    "outputId": "eec95f36-d6c3-444d-dc0d-0b7161d31a9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.9983219504356384\n",
      "Precision: 0.5199970006942749\n",
      "Recall: 0.6185795664787292\n"
     ]
    }
   ],
   "source": [
    "TRESHOLD = 0.05\n",
    "y_pred = 1*(pred_prob > TRESHOLD)\n",
    "print('Exact Match:', exact_match(target, y_pred))\n",
    "print('Precision:', precision(target, y_pred))\n",
    "print('Recall:', recall(target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mokX9IVM_zj",
    "outputId": "239c1246-ca6d-4579-be5b-5ebebb4fe526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 0.9983181357383728\n",
      "Precision: 0.00599716417491436\n",
      "Recall: 0.005377625115215778\n"
     ]
    }
   ],
   "source": [
    "TRESHOLD = 0.9\n",
    "y_pred = 1*(pred_prob > TRESHOLD)\n",
    "\n",
    "print('Exact Match:', exact_match(target, y_pred))\n",
    "print('Precision:', precision(target, y_pred))\n",
    "print('Recall:', recall(target, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUWFt7jjM_zj"
   },
   "source": [
    "Дальше мы будем строить довольно много прогнозов. Давайте напишем код для их строительства в виде функции. Обратите внимание, что на модели со снипетом она упадёт. Когда вы доберётесь до строительства прогнозов, функцию придётся немного модернизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "B3YFX4d2M_zk"
   },
   "outputs": [],
   "source": [
    "def get_predict(model, dataset):\n",
    "    dataloader = DataLoader(dataset, shuffle=False, batch_size=len(dataset))\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        if len(batch) == 2:\n",
    "            title, target = batch\n",
    "            logit = model(title)\n",
    "        elif len(batch) == 3:\n",
    "            title, snippet, target = batch\n",
    "            logit = model(title, snippet)\n",
    "        \n",
    "        pred_prob = F.softmax(logit, dim=1)\n",
    "    \n",
    "    assert pred_prob.shape[0] == len(dataset)\n",
    "    return pred_prob, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0n493n_M_zk"
   },
   "source": [
    "__[0.5 балла] Задание 5:__ Какая метрика для нас в этой задаче важнее? Точность или полнота? Почему?\n",
    "\n",
    "__ваш ответ:__ в данной задаче важнее полнота, так как нам нужно каждая статья правильно классифицирована"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TFEvxI6M_zk"
   },
   "source": [
    "- Напишите функцию, которая будет подбирать оптимальное значение порога, оптимизирующее выбранную вами метрику.\n",
    "- Подберите значение порога на валидационной выборке.\n",
    "- Сравните модель со сниппетами и без сниппетов, используя выбранную вами метрику при оптимальном значении порога на тестовой выборке.\n",
    "- Какая из них оказалась лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "BCq67JmUM_zl"
   },
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_probs, metric='f1', step=0.01):\n",
    "    thresholds = np.arange(0.0, 1.0, step)\n",
    "    best_threshold = 0.0\n",
    "    best_metric = -1.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_probs > threshold).float()\n",
    "        if metric == 'recall':\n",
    "            current_metric = recall(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            current_metric = precision(y_true, y_pred)\n",
    "        elif metric == 'f1':\n",
    "            current_metric = f1_score(y_true, y_pred)\n",
    "        \n",
    "        if current_metric > best_metric:\n",
    "            best_metric = current_metric\n",
    "            best_threshold = threshold\n",
    "            \n",
    "    return best_threshold, best_metric\n",
    "\n",
    "def f1_score(target, y_pred):\n",
    "    precision_val = precision(target, y_pred)\n",
    "    recall_val = recall(target, y_pred)\n",
    "    if precision_val + recall_val == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision_val * recall_val) / (precision_val + recall_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальный порог для базовой модели: 0.06, F1-Score: 0.6522\n",
      "Оптимальный порог для модели со сниппетом: 0.06, F1-Score: 0.6660\n"
     ]
    }
   ],
   "source": [
    "val_probs_baseline, val_targets_baseline = get_predict(model_baseline, val_dataset)\n",
    "\n",
    "val_probs_snippet, val_targets_snippet = get_predict(model_snippet, val_dataset_snippet)\n",
    "\n",
    "optimal_threshold_baseline, best_f1_baseline = find_optimal_threshold(\n",
    "    val_targets_baseline, \n",
    "    val_probs_baseline, \n",
    "    metric='f1', \n",
    "    step=0.01\n",
    ")\n",
    "\n",
    "print(f\"Оптимальный порог для базовой модели: {optimal_threshold_baseline:.2f}, F1-Score: {best_f1_baseline:.4f}\")\n",
    "\n",
    "optimal_threshold_snippet, best_f1_snippet = find_optimal_threshold(\n",
    "    val_targets_snippet, \n",
    "    val_probs_snippet, \n",
    "    metric='f1', \n",
    "    step=0.01\n",
    ")\n",
    "\n",
    "print(f\"Оптимальный порог для модели со сниппетом: {optimal_threshold_snippet:.2f}, F1-Score: {best_f1_snippet:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall на тестовой выборке для базовой модели: 0.5959\n",
      "Recall на тестовой выборке для модели со сниппетом: 0.6010\n"
     ]
    }
   ],
   "source": [
    "def apply_threshold(y_probs, threshold):\n",
    "    return (y_probs > threshold).float()\n",
    "\n",
    "test_probs_baseline, test_targets_baseline = get_predict(model_baseline, test_dataset)\n",
    "y_pred_baseline = apply_threshold(test_probs_baseline, optimal_threshold_baseline)\n",
    "recall_baseline_test = recall(test_targets_baseline, y_pred_baseline)\n",
    "print(f\"Recall на тестовой выборке для базовой модели: {recall_baseline_test:.4f}\")\n",
    "\n",
    "test_probs_snippet, test_targets_snippet = get_predict(model_snippet, test_dataset_snippet)\n",
    "y_pred_snippet = apply_threshold(test_probs_snippet, optimal_threshold_snippet)\n",
    "recall_snippet_test = recall(test_targets_snippet, y_pred_snippet)\n",
    "print(f\"Recall на тестовой выборке для модели со сниппетом: {recall_snippet_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Базовая модель на тестовой выборке:\n",
      "F1-Score: 0.5669\n",
      "Precision: 0.5406\n",
      "Recall: 0.5959\n",
      "\n",
      "Модель со сниппетом на тестовой выборке:\n",
      "F1-Score: 0.5771\n",
      "Precision: 0.5551\n",
      "Recall: 0.6010\n"
     ]
    }
   ],
   "source": [
    "def apply_threshold(y_probs, threshold):\n",
    "    return (y_probs > threshold).float()\n",
    "\n",
    "test_probs_baseline, test_targets_baseline = get_predict(model_baseline, test_dataset)\n",
    "y_pred_baseline = apply_threshold(test_probs_baseline, optimal_threshold_baseline)\n",
    "recall_baseline_test = recall(test_targets_baseline, y_pred_baseline)\n",
    "precision_baseline_test = precision(test_targets_baseline, y_pred_baseline)\n",
    "f1_baseline_test = f1_score(test_targets_baseline, y_pred_baseline)\n",
    "\n",
    "print(f\"Базовая модель на тестовой выборке:\")\n",
    "print(f\"F1-Score: {f1_baseline_test:.4f}\")\n",
    "print(f\"Precision: {precision_baseline_test:.4f}\")\n",
    "print(f\"Recall: {recall_baseline_test:.4f}\")\n",
    "\n",
    "test_probs_snippet, test_targets_snippet = get_predict(model_snippet, test_dataset_snippet)\n",
    "y_pred_snippet = apply_threshold(test_probs_snippet, optimal_threshold_snippet)\n",
    "recall_snippet_test = recall(test_targets_snippet, y_pred_snippet)\n",
    "precision_snippet_test = precision(test_targets_snippet, y_pred_snippet)\n",
    "f1_snippet_test = f1_score(test_targets_snippet, y_pred_snippet)\n",
    "\n",
    "print(f\"\\nМодель со сниппетом на тестовой выборке:\")\n",
    "print(f\"F1-Score: {f1_snippet_test:.4f}\")\n",
    "print(f\"Precision: {precision_snippet_test:.4f}\")\n",
    "print(f\"Recall: {recall_snippet_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Со снипитом лучше, и пресижн выше и рекол выше.\n",
    "\n",
    "UPD: Я поперезапускал с не фиксированным сидом, и то со снипетом то без лучше, так что можно сказать примерно одинаково\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-QRc-UXM_zl"
   },
   "source": [
    "__[0.5 балла] Задание 6:__  Постройте прогнозы для отложенной выборки, которая представляет из себя пересечение сайта РИА-новостей и ВКонтакте. Проседает ли на ней качество модели? Насколько сильно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3Tc02SgLM_zl",
    "outputId": "5525c33a-b896-47bb-f9d5-9fb9c87f06f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>href</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>target_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/20181231/1548960744.html</td>\n",
       "      <td>митрополит рязанский раскритиковал иронию судьбы</td>\n",
       "      <td>[1115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/20181231/1548958617.html</td>\n",
       "      <td>на украине позавидовали стене на границе с крымом</td>\n",
       "      <td>[1359, 942]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/20181231/1548957394.html</td>\n",
       "      <td>в госдуме предложили отказаться от газа в жилы...</td>\n",
       "      <td>[1556, 1556, 1201, 194, 973]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/20181231/1548954909.html</td>\n",
       "      <td>названы лучшие средства от похмелья</td>\n",
       "      <td>[1115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/20181231/1548957120.html</td>\n",
       "      <td>новогоднее поздравление порошенко разозлило по...</td>\n",
       "      <td>[1359, 942]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        href  \\\n",
       "0  /20181231/1548960744.html   \n",
       "1  /20181231/1548958617.html   \n",
       "2  /20181231/1548957394.html   \n",
       "3  /20181231/1548954909.html   \n",
       "4  /20181231/1548957120.html   \n",
       "\n",
       "                                         title_clean  \\\n",
       "0   митрополит рязанский раскритиковал иронию судьбы   \n",
       "1  на украине позавидовали стене на границе с крымом   \n",
       "2  в госдуме предложили отказаться от газа в жилы...   \n",
       "3                названы лучшие средства от похмелья   \n",
       "4  новогоднее поздравление порошенко разозлило по...   \n",
       "\n",
       "                    target_tags  \n",
       "0                        [1115]  \n",
       "1                   [1359, 942]  \n",
       "2  [1556, 1556, 1201, 194, 973]  \n",
       "3                        [1115]  \n",
       "4                   [1359, 942]  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        href  \\\n",
      "0  /20181231/1548960744.html   \n",
      "1  /20181231/1548958617.html   \n",
      "2  /20181231/1548957394.html   \n",
      "3  /20181231/1548954909.html   \n",
      "4  /20181231/1548957120.html   \n",
      "\n",
      "                                         title_clean  \\\n",
      "0   митрополит рязанский раскритиковал иронию судьбы   \n",
      "1  на украине позавидовали стене на границе с крымом   \n",
      "2  в госдуме предложили отказаться от газа в жилы...   \n",
      "3                названы лучшие средства от похмелья   \n",
      "4  новогоднее поздравление порошенко разозлило по...   \n",
      "\n",
      "                    target_tags snippet_clean  \n",
      "0                        [1115]          unkn  \n",
      "1                   [1359, 942]          unkn  \n",
      "2  [1556, 1556, 1201, 194, 973]          unkn  \n",
      "3                        [1115]          unkn  \n",
      "4                   [1359, 942]          unkn  \n"
     ]
    }
   ],
   "source": [
    "df_oob = df_oob.merge(\n",
    "    df_vk[['href', 'snippet_clean']],\n",
    "    on='href',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_oob['snippet_clean'] = df_oob['snippet_clean'].fillna(\"#UNKN\").apply(normalise_text)\n",
    "\n",
    "print(df_oob.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "1Z5AvLtcM_zl"
   },
   "outputs": [],
   "source": [
    "class OOBDatasetBaseline(Dataset):\n",
    "    def __init__(self, titles, targets, vocab, vocab_size, max_title_len, max_classes):\n",
    "        self.vocab = {word: idx for word, idx in vocab.items() if idx < vocab_size}\n",
    "        self.max_classes = max_classes\n",
    "        self.y = self.target_ohe(targets)\n",
    "        self.X_title = self.create_text(titles, max_title_len)\n",
    "    \n",
    "    def target_ohe(self, target):\n",
    "        y = torch.zeros((len(target), self.max_classes))\n",
    "        for i, t in enumerate(target):\n",
    "            y[[i]*len(t), t] = 1.0\n",
    "        return y\n",
    "    \n",
    "    def create_text(self, texts, max_len):\n",
    "        result = []\n",
    "        for sent in texts:\n",
    "            sent_tokenize = [self.vocab.get(item, 1) for item in word_tokenize(sent)]\n",
    "            if len(sent_tokenize) >= max_len:\n",
    "                sent_tokenize = sent_tokenize[:max_len]\n",
    "            else:\n",
    "                sent_tokenize += [0] * (max_len - len(sent_tokenize))\n",
    "            result.append(sent_tokenize)\n",
    "        return torch.tensor(result, dtype=torch.int)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_title)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X_title[idx, :], self.y[idx])\n",
    "\n",
    "class OOBDatasetSnippet(Dataset):\n",
    "    def __init__(self, titles, snippets, targets, vocab, vocab_size, max_title_len, max_snippet_len, max_classes):\n",
    "        self.vocab = {word: idx for word, idx in vocab.items() if idx < vocab_size}\n",
    "        self.max_classes = max_classes\n",
    "        self.y = self.target_ohe(targets)\n",
    "        self.X_title = self.create_text(titles, max_title_len)\n",
    "        self.X_snippet = self.create_text(snippets, max_snippet_len)\n",
    "    \n",
    "    def target_ohe(self, target):\n",
    "        y = torch.zeros((len(target), self.max_classes))\n",
    "        for i, t in enumerate(target):\n",
    "            y[[i]*len(t), t] = 1.0\n",
    "        return y\n",
    "    \n",
    "    def create_text(self, texts, max_len):\n",
    "        result = []\n",
    "        for sent in texts:\n",
    "            sent_tokenize = [self.vocab.get(item, 1) for item in word_tokenize(sent)]\n",
    "            if len(sent_tokenize) >= max_len:\n",
    "                sent_tokenize = sent_tokenize[:max_len]\n",
    "            else:\n",
    "                sent_tokenize += [0] * (max_len - len(sent_tokenize))\n",
    "            result.append(sent_tokenize)\n",
    "        return torch.tensor(result, dtype=torch.int)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_title)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X_title[idx, :], self.X_snippet[idx, :], self.y[idx])\n",
    "\n",
    "oob_dataset_baseline = OOBDatasetBaseline(\n",
    "    titles=df_oob.title_clean.values,\n",
    "    targets=df_oob.target_tags.values,\n",
    "    vocab=vocabulary,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_title_len=MAX_TITLE_LEN,\n",
    "    max_classes=CLASSES_NUM\n",
    ")\n",
    "\n",
    "oob_dataset_snippet = OOBDatasetSnippet(\n",
    "    titles=df_oob.title_clean.values,\n",
    "    snippets=df_oob.snippet_clean.values,\n",
    "    targets=df_oob.target_tags.values,\n",
    "    vocab=vocabulary,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_title_len=MAX_TITLE_LEN,\n",
    "    max_snippet_len=MAX_SNIPPET_LEN,\n",
    "    max_classes=CLASSES_NUM\n",
    ")\n",
    "\n",
    "oob_dataloader_baseline = DataLoader(oob_dataset_baseline, shuffle=False, batch_size=len(oob_dataset_baseline), num_workers=4)\n",
    "oob_dataloader_snippet = DataLoader(oob_dataset_snippet, shuffle=False, batch_size=len(oob_dataset_snippet), num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "oob_probs_baseline, oob_targets_baseline = get_predict(model_baseline, oob_dataset_baseline)\n",
    "\n",
    "oob_probs_snippet, oob_targets_snippet = get_predict(model_snippet, oob_dataset_snippet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Базовая модель на отложенной выборке:\n",
      "F1-Score: 0.5316\n",
      "Precision: 0.5014\n",
      "Recall: 0.5656\n",
      "\n",
      "Модель со сниппетом на отложенной выборке:\n",
      "F1-Score: 0.2546\n",
      "Precision: 0.2513\n",
      "Recall: 0.2580\n"
     ]
    }
   ],
   "source": [
    "def apply_threshold(y_probs, threshold):\n",
    "    return (y_probs > threshold).float()\n",
    "\n",
    "y_pred_baseline_oob = apply_threshold(oob_probs_baseline, optimal_threshold_baseline)\n",
    "y_pred_snippet_oob = apply_threshold(oob_probs_snippet, optimal_threshold_snippet)\n",
    "\n",
    "recall_baseline_oob = recall(oob_targets_baseline, y_pred_baseline_oob)\n",
    "precision_baseline_oob = precision(oob_targets_baseline, y_pred_baseline_oob)\n",
    "f1_baseline_oob = f1_score(oob_targets_baseline, y_pred_baseline_oob)\n",
    "\n",
    "recall_snippet_oob = recall(oob_targets_snippet, y_pred_snippet_oob)\n",
    "precision_snippet_oob = precision(oob_targets_snippet, y_pred_snippet_oob)\n",
    "f1_snippet_oob = f1_score(oob_targets_snippet, y_pred_snippet_oob)\n",
    "\n",
    "print(f\"Базовая модель на отложенной выборке:\")\n",
    "print(f\"F1-Score: {f1_baseline_oob:.4f}\")\n",
    "print(f\"Precision: {precision_baseline_oob:.4f}\")\n",
    "print(f\"Recall: {recall_baseline_oob:.4f}\")\n",
    "\n",
    "print(f\"\\nМодель со сниппетом на отложенной выборке:\")\n",
    "print(f\"F1-Score: {f1_snippet_oob:.4f}\")\n",
    "print(f\"Precision: {precision_snippet_oob:.4f}\")\n",
    "print(f\"Recall: {recall_snippet_oob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лог эксперементов https://api.wandb.ai/links/dima-malinitskiy-hse-university/tvjbhhwo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmFzWAHtM_zl"
   },
   "source": [
    "Давайте модернизируем наши архитектуры настолько, насколько это возможно.\n",
    "\n",
    "__[0.9 балла] Бонусное задание 1:__ Попробуйте собрать более большую архитектуру. Например, сразу после слоя эмбеддингов вы можете попробовать добавить свёрточные слои (`Conv1D` свёртки). Поиграйте с оптимизатором и тп.\n",
    "\n",
    "Опишите результаты своих экспериментов ниже. Расскажите, что конкретно вы делали и удалось ли вам улучшить качество модели. Все траектории обучения залоггируйте на WandB.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obvk588CM_zl"
   },
   "source": [
    "__Ваш лог экспериментов:__\n",
    "\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mII3wWHOM_zl"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# (⊙_⊙)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kMKmn72M_zl"
   },
   "source": [
    "__[0.9 балла] Бонусное задание 2:__ Скачайте с сайта [Rusvectores](https://rusvectores.org/ru/models/) любые новостные word2vec эмбединги. Возьмите из модели эмбеддинги для всех слов, которые встречаются вв вашем словаре и добавьте их в модель первым слоем. Заморозьте этот слой и не обновляйте в нём веса. Если у вас в словаре есть слово, но его нет среди предобученных эмбеддингов, замените его на токен `#UNKN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ldlZ5gM_zl"
   },
   "source": [
    "__Ваш лог экспериментов:__\n",
    "\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "XoIlDPe0M_zl"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# =^･ｪ･^="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8ziz2ATM_zl"
   },
   "source": [
    "__[1.5 балла] Задание 7:__ Зафайнтьюньте трансформер для решения задачи с помощью библиотеки `hugging face`. Выбор предобученной модели кратко обоснуйте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я выбрал DeepPavlov/rubert-base-cased. На hugging face, по запросу russian news data, нашел эту модельку, и понял что под эту задачу она очень хорошо подходит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoVv-AS4M_zl"
   },
   "source": [
    "__Ваш лог экспериментов:__\n",
    "\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg02xRLrM_zl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset_hf = Dataset.from_pandas(df_train[['title_clean', 'snippet_clean', 'target_tags']])\n",
    "val_dataset_hf = Dataset.from_pandas(df_val[['title_clean', 'snippet_clean', 'target_tags']])\n",
    "test_dataset_hf = Dataset.from_pandas(df_test[['title_clean', 'snippet_clean', 'target_tags']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 136875/136875 [00:33<00:00, 4130.95 examples/s]\n",
      "Map: 100%|██████████| 43258/43258 [00:07<00:00, 5691.01 examples/s]\n",
      "Map: 100%|██████████| 20176/20176 [00:02<00:00, 8290.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['title_clean'],\n",
    "        examples['snippet_clean'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_dataset_hf = train_dataset_hf.map(tokenize_function, batched=True)\n",
    "val_dataset_hf = val_dataset_hf.map(tokenize_function, batched=True)\n",
    "test_dataset_hf = test_dataset_hf.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=CLASSES_NUM,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "Map: 100%|██████████| 136875/136875 [00:32<00:00, 4176.29 examples/s]\n",
      "Map: 100%|██████████| 43258/43258 [00:13<00:00, 3251.89 examples/s] \n",
      "Map: 100%|██████████| 20176/20176 [00:02<00:00, 8913.41 examples/s]\n",
      "Map: 100%|██████████| 136875/136875 [00:01<00:00, 87582.23 examples/s]\n",
      "Map: 100%|██████████| 43258/43258 [00:00<00:00, 97921.30 examples/s] \n",
      "Map: 100%|██████████| 20176/20176 [00:00<00:00, 134597.72 examples/s]\n",
      "/home/user/myenv/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_15604/2856611629.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25665' max='25665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25665/25665 1:36:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.606965</td>\n",
       "      <td>0.628866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.005239</td>\n",
       "      <td>0.801242</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.712707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.771277</td>\n",
       "      <td>0.721393</td>\n",
       "      <td>0.745501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002748664002865553, 'eval_precision': 0.7010309278350515, 'eval_recall': 0.8717948717948718, 'eval_f1': 0.7771428571428571, 'eval_runtime': 75.6758, 'eval_samples_per_second': 266.611, 'eval_steps_per_second': 4.176, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb_key import API_KEY \n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "wandb.login(key=API_KEY)\n",
    "\n",
    "train_dataset_hf = Dataset.from_pandas(df_train[['title_clean', 'snippet_clean', 'target_tags']])\n",
    "val_dataset_hf = Dataset.from_pandas(df_val[['title_clean', 'snippet_clean', 'target_tags']])\n",
    "test_dataset_hf = Dataset.from_pandas(df_test[['title_clean', 'snippet_clean', 'target_tags']])\n",
    "\n",
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['title_clean'],\n",
    "        examples['snippet_clean'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_dataset_hf = train_dataset_hf.map(tokenize_function, batched=True)\n",
    "val_dataset_hf = val_dataset_hf.map(tokenize_function, batched=True)\n",
    "test_dataset_hf = test_dataset_hf.map(tokenize_function, batched=True)\n",
    "\n",
    "CLASSES_NUM = len(idx2tag)\n",
    "num_labels = CLASSES_NUM\n",
    "\n",
    "def encode_labels(examples):\n",
    "    labels = np.zeros((len(examples['target_tags']), num_labels))\n",
    "    for i, tags in enumerate(examples['target_tags']):\n",
    "        for tag in tags:\n",
    "            if tag < num_labels:\n",
    "                labels[i][tag] = 1\n",
    "    examples[\"labels\"] = labels.tolist()\n",
    "    return examples\n",
    "\n",
    "train_dataset_hf = train_dataset_hf.map(encode_labels, batched=True)\n",
    "val_dataset_hf = val_dataset_hf.map(encode_labels, batched=True)\n",
    "test_dataset_hf = test_dataset_hf.map(encode_labels, batched=True)\n",
    "\n",
    "train_dataset_hf = train_dataset_hf.remove_columns(['title_clean', 'snippet_clean', 'target_tags'])\n",
    "val_dataset_hf = val_dataset_hf.remove_columns(['title_clean', 'snippet_clean', 'target_tags'])\n",
    "test_dataset_hf = test_dataset_hf.remove_columns(['title_clean', 'snippet_clean', 'target_tags'])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    y_pred = (probs > 0.5).int().numpy()\n",
    "    y_true = labels\n",
    "    precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_hf,\n",
    "    eval_dataset=val_dataset_hf,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset_hf)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt8mcUllUmM8"
   },
   "source": [
    "Сравните все обученные модели между собой на тестовой выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoCALHfJM_zm"
   },
   "source": [
    "## Часть 2: предсказание категорий (0.5 балла)\n",
    "\n",
    "**[0.5 балла] Задание 8:** Возьмите датасет `df_vk` и для всех новостей из него предскажите категории с помощью лучшей, получившейся у вас модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'href', 'datetime', 'title', 'likes', 'comments', 'snippet',\n",
      "       'title_clean', 'snippet_clean'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_vk.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99fmOzZUM_zm"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_vk['target_tags'] = [[] for _ in range(df_vk.shape[0])]\n",
    "\n",
    "vk_dataset_hf = Dataset.from_pandas(df_vk[['title_clean', 'snippet_clean', 'target_tags']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title_clean', 'snippet_clean', 'target_tags']\n"
     ]
    }
   ],
   "source": [
    "print(vk_dataset_hf.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 19928/19928 [00:01<00:00, 12236.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_function_infer(examples):\n",
    "    return tokenizer(\n",
    "        examples['title_clean'],\n",
    "        examples['snippet_clean'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "vk_dataset_hf = vk_dataset_hf.map(tokenize_function_infer, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(vk_dataset_hf)\n",
    "logits = predictions.predictions\n",
    "probs = torch.sigmoid(torch.tensor(logits))\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "y_pred = (probs > threshold).int().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpw07S0UM_zm"
   },
   "source": [
    "На всякий случай сохраните табличку с получившимися у вас предсказаниями. Мало ли, вы не доделаете последнее задание, а потом захотите вернуться к нему. Не прогонять же обучение нейросети и инференс по второму кругу..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alIF9U2tM_zm"
   },
   "outputs": [],
   "source": [
    "def decode_predictions(y_pred, idx2tag):\n",
    "    predictions = []\n",
    "    for sample in y_pred:\n",
    "        tags = [idx2tag[idx] for idx, val in enumerate(sample) if val == 1]\n",
    "        predictions.append(tags)\n",
    "    return predictions\n",
    "\n",
    "predicted_tags = decode_predictions(y_pred, idx2tag)\n",
    "\n",
    "df_vk['target_tags'] = predicted_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vk.to_csv('vk_news_with_predictions.csv', index=False)\n",
    "df_vk.to_parquet('vk_news_with_predictions.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_jo72yzM_zm"
   },
   "source": [
    "## Продолжение дальше, в некст файле"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0029cbbc48c64effb57bffff5d199e34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0140e6c8951147b6a7e079682e938e7f",
      "placeholder": "​",
      "style": "IPY_MODEL_534e121744814f4d8880072004513e37",
      "value": " 11/11 [00:00&lt;00:00, 19.45it/s]"
     }
    },
    "0140e6c8951147b6a7e079682e938e7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09093bb710bd4903abf2e10db6efc0a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fc5cc9d0df74d5081fa3f8c89595daf",
      "placeholder": "​",
      "style": "IPY_MODEL_0cbe638b1d354bea83ccba54ee3d1f5c",
      "value": " 11/11 [00:00&lt;00:00, 32.16it/s]"
     }
    },
    "0a54156fabcd4ef3b73b038b71485a6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cbe638b1d354bea83ccba54ee3d1f5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "100219bd594645c5b60c3da4a44344fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13a1fd50c8f24f12a0f28485b6168606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17d8af2b171440cba00f8b2df1a08073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_201a41be363549eaa0fdc903fb41f83e",
      "placeholder": "​",
      "style": "IPY_MODEL_60c4aa6c4c8145949cd16ec0670227a1",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "17e62da8657f44d9a998f4acbbd29671": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ae57433e573436b8bbdf6c22630930d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d422f8cbd47402bb8f28b258e5abed0",
      "placeholder": "​",
      "style": "IPY_MODEL_80d177b53a5f4fe0a733d3b7d36e9b30",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "1ae6951f8b984b318210df9d009479d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9456651273140f9bd6e587ab852eeec",
       "IPY_MODEL_b174cb3407a04b2989e6049555265446",
       "IPY_MODEL_7bd174d7cd2346be9db9d5a0a170f8cb"
      ],
      "layout": "IPY_MODEL_3ada921ee7644de997b57ffd2c57044d"
     }
    },
    "1c10405770654b90a3635ba4acabb6ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "201a41be363549eaa0fdc903fb41f83e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "286fbf1303dc4f0b9c1bb3cfb34f920e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a38ec8f52d74e018a4c1876d3a2f874": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd22c38d837a404ab892ddaddf4a8988",
       "IPY_MODEL_a474d8d377674579b3c9f5327fbb1862",
       "IPY_MODEL_0029cbbc48c64effb57bffff5d199e34"
      ],
      "layout": "IPY_MODEL_e9d2f22400db472283508e8c01f08fbb"
     }
    },
    "2aa0bc933aae46729672d55e60c17616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_546d1deb5c934916b722fb9ab0ce5e67",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_86ed101f86384b9b92eb3faccae5bca2",
      "value": 11
     }
    },
    "2cb3f56ff669424da4e9bace352831b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "2d71626a24cb45f39095e7e3fd621707": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fa8591a1a834d8785b1ebe8eff2b10d",
      "placeholder": "​",
      "style": "IPY_MODEL_acfec13aec454320b585c0b8331e9448",
      "value": " 2/2 [00:00&lt;00:00, 14.02it/s]"
     }
    },
    "2d98065fccb34cb1b44740aa4510f0e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6072601881c341fb8e5329fab05ee52d",
      "placeholder": "​",
      "style": "IPY_MODEL_13a1fd50c8f24f12a0f28485b6168606",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "2da5b84f9384445a8d9db58ef3dafa5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e02be5bf0be40bf87f1364b1fd587ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f4dcf310c634bdba125e8bf46d3db26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "3ada921ee7644de997b57ffd2c57044d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "3dc20b49832e46c6ad70dc754f43555f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ef009aeabae43b48bc913a592d1a019": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75b99a050a37493389ff96724a9a3b72",
       "IPY_MODEL_78fe9e3a3d614de9a6f3efacde67d247",
       "IPY_MODEL_2d71626a24cb45f39095e7e3fd621707"
      ],
      "layout": "IPY_MODEL_4f75594be29e49cc99fe61a6787e7564"
     }
    },
    "46204f92abf1414da360d79b4ca7a924": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f75594be29e49cc99fe61a6787e7564": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "515318593400421487560182023801f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "534e121744814f4d8880072004513e37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "546d1deb5c934916b722fb9ab0ce5e67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "560450b99c8f4636b3a700db3fc21253": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "58fbdeafd7ee40cd9392cd4a62c49b81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d422f8cbd47402bb8f28b258e5abed0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5db675de6b8348e9a9823384541bdfff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6072601881c341fb8e5329fab05ee52d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60c4aa6c4c8145949cd16ec0670227a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6351ca36d8d648afa472093df7e73e45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a6a637bb7664b4495b2b56b45d88ee4",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74bf3b2a873e40ffa139f1afe652ffe8",
      "value": 11
     }
    },
    "640ac5a06820498585c48a1b69355d0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6769dc278aae487db30d78caa97c6dd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71c435b7fa6f4a0fb44a4c8a8b5a73f6",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c05c8b6064f94652a1f2ed5c3bb7c2b0",
      "value": 11
     }
    },
    "6fa8591a1a834d8785b1ebe8eff2b10d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71c435b7fa6f4a0fb44a4c8a8b5a73f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74bf3b2a873e40ffa139f1afe652ffe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "75b99a050a37493389ff96724a9a3b72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ab912602cc24346a011fdf58feafbde",
      "placeholder": "​",
      "style": "IPY_MODEL_af211f3be2ce4fb1b6733fa552ed041b",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "78fe9e3a3d614de9a6f3efacde67d247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84f32a71a7cd4de6b55a9558717d00ef",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5db675de6b8348e9a9823384541bdfff",
      "value": 2
     }
    },
    "7a6a637bb7664b4495b2b56b45d88ee4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bb5cc7b77654cc0b27a22552941a1ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bd174d7cd2346be9db9d5a0a170f8cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2da5b84f9384445a8d9db58ef3dafa5e",
      "placeholder": "​",
      "style": "IPY_MODEL_abcb57f0947f413d9cb83c62c6529562",
      "value": " 2139/2139 [00:17&lt;00:00, 122.83it/s, v_num=1, train_loss=7.280, val_loss=10.70]"
     }
    },
    "7dc052c589534aa986569d863596a2a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "80d177b53a5f4fe0a733d3b7d36e9b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84f32a71a7cd4de6b55a9558717d00ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86ed101f86384b9b92eb3faccae5bca2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fc5cc9d0df74d5081fa3f8c89595daf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "938d3da0eef54ee997fc0ce7ccf1807b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "950950a569914ad1a263f3f295e57852": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cba28fcac7c349e8ac9794d507768744",
      "placeholder": "​",
      "style": "IPY_MODEL_7bb5cc7b77654cc0b27a22552941a1ce",
      "value": " 11/11 [00:00&lt;00:00, 23.50it/s]"
     }
    },
    "9ab912602cc24346a011fdf58feafbde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ec11067fc81414d97457b47e93b78a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dc20b49832e46c6ad70dc754f43555f",
      "placeholder": "​",
      "style": "IPY_MODEL_100219bd594645c5b60c3da4a44344fe",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "a2903f89f4584f16b9f5b412d2730672": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a474d8d377674579b3c9f5327fbb1862": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e02be5bf0be40bf87f1364b1fd587ec",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_560450b99c8f4636b3a700db3fc21253",
      "value": 11
     }
    },
    "abcb57f0947f413d9cb83c62c6529562": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acfec13aec454320b585c0b8331e9448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af211f3be2ce4fb1b6733fa552ed041b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b174cb3407a04b2989e6049555265446": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_938d3da0eef54ee997fc0ce7ccf1807b",
      "max": 2139,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c10405770654b90a3635ba4acabb6ef",
      "value": 2139
     }
    },
    "b23fac677f5e4e8884f84fe60585330d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ae57433e573436b8bbdf6c22630930d",
       "IPY_MODEL_6769dc278aae487db30d78caa97c6dd8",
       "IPY_MODEL_b5b6d56829af44d99b904176b9552101"
      ],
      "layout": "IPY_MODEL_7dc052c589534aa986569d863596a2a3"
     }
    },
    "b5b6d56829af44d99b904176b9552101": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c44756a0ec83400f9f6daf8d1179ad91",
      "placeholder": "​",
      "style": "IPY_MODEL_a2903f89f4584f16b9f5b412d2730672",
      "value": " 11/11 [00:00&lt;00:00, 27.27it/s]"
     }
    },
    "b6179f72e8f94772a4e1e2a503bc1d47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9ec11067fc81414d97457b47e93b78a1",
       "IPY_MODEL_6351ca36d8d648afa472093df7e73e45",
       "IPY_MODEL_d1efd4fe9adc4243a81d401784b77530"
      ],
      "layout": "IPY_MODEL_2cb3f56ff669424da4e9bace352831b8"
     }
    },
    "be93fd003ef54d7e85bb1fcaa894e467": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c05c8b6064f94652a1f2ed5c3bb7c2b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c44756a0ec83400f9f6daf8d1179ad91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cba28fcac7c349e8ac9794d507768744": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1efd4fe9adc4243a81d401784b77530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_640ac5a06820498585c48a1b69355d0d",
      "placeholder": "​",
      "style": "IPY_MODEL_515318593400421487560182023801f1",
      "value": " 11/11 [00:00&lt;00:00, 15.17it/s]"
     }
    },
    "d8cd82c7af8747148b5b4750e8f1372a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17d8af2b171440cba00f8b2df1a08073",
       "IPY_MODEL_fde8213392c546b58ad8a81aacf4fcb3",
       "IPY_MODEL_09093bb710bd4903abf2e10db6efc0a0"
      ],
      "layout": "IPY_MODEL_d92ec28e7e6c47c4b2e3669b502b8b9e"
     }
    },
    "d92ec28e7e6c47c4b2e3669b502b8b9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "d9456651273140f9bd6e587ab852eeec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17e62da8657f44d9a998f4acbbd29671",
      "placeholder": "​",
      "style": "IPY_MODEL_be93fd003ef54d7e85bb1fcaa894e467",
      "value": "Epoch 4: 100%"
     }
    },
    "e9d2f22400db472283508e8c01f08fbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "f2b6c28c36e64f66b54834f543cec282": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d98065fccb34cb1b44740aa4510f0e7",
       "IPY_MODEL_2aa0bc933aae46729672d55e60c17616",
       "IPY_MODEL_950950a569914ad1a263f3f295e57852"
      ],
      "layout": "IPY_MODEL_2f4dcf310c634bdba125e8bf46d3db26"
     }
    },
    "fd22c38d837a404ab892ddaddf4a8988": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46204f92abf1414da360d79b4ca7a924",
      "placeholder": "​",
      "style": "IPY_MODEL_0a54156fabcd4ef3b73b038b71485a6a",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "fde8213392c546b58ad8a81aacf4fcb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58fbdeafd7ee40cd9392cd4a62c49b81",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_286fbf1303dc4f0b9c1bb3cfb34f920e",
      "value": 11
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
