{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "962eeb72-0836-4fb1-80cd-154f7d5a111f",
      "metadata": {
        "id": "962eeb72-0836-4fb1-80cd-154f7d5a111f"
      },
      "source": [
        "# DQN (5 баллов)\n",
        "\n",
        "Метод обучения DQN — это нейросетевая адаптация алгоритма Q-learning. Также для него разработан набор дополнений, которые становятся актуальными при переходе к обучению глубоких нейронных сетей и решению более сложных задач (то есть задач с бОльшим пространством состояний).\n",
        "\n",
        "Реализуем алгоритм DQN для решения среды [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), цель которой балансировать палочкой в вертикальном положении, управляя только тележкой, к которой она прикреплена. Будем использовать библиотеку PyTorch для обучения нейронной сети, аппроксимирующей Q-функцию (но вы можете воспользоваться и любой другой библиотекой для обучения глубоких сетей, таких как TensorFlow или Jax).\n",
        "\n",
        "![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
        "\n",
        "![cartpole](https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "aa4a81a2-b188-47be-8b99-66a95401eed4",
      "metadata": {
        "id": "aa4a81a2-b188-47be-8b99-66a95401eed4"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import base64\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pygame\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import WrapperSpec\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "691b1f14",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24bcf642-10cc-431b-b9b2-18bc2bfbf1a3",
      "metadata": {
        "id": "24bcf642-10cc-431b-b9b2-18bc2bfbf1a3"
      },
      "source": [
        "### Пространство действий\n",
        "\n",
        "Действие представляет собой `ndarray` формы `(1,)`, принимающее значения из множества `{0, 1}`, указывающие направление постоянной силы, приложенной к тележке:\n",
        "\n",
        "- **0**: толкнуть тележку влево  \n",
        "- **1**: толкнуть тележку вправо  \n",
        "\n",
        "**Примечание:** величина изменения скорости тележки не фиксирована и зависит от угла наклона шеста. Положение центра тяжести шеста влияет на количество энергии, требующейся для перемещения тележки под ним.\n",
        "\n",
        "---\n",
        "\n",
        "### Пространство наблюдений\n",
        "\n",
        "Наблюдение — это `ndarray` формы `(4,)`, элементы которого соответствуют следующим величинам:\n",
        "\n",
        "| №  | Наблюдение              | Минимум            | Максимум           |\n",
        "|----|-------------------------|--------------------|--------------------|\n",
        "| 0  | Положение тележки       | -4.8               | 4.8                |\n",
        "| 1  | Скорость тележки        | -∞                 | +∞                 |\n",
        "| 2  | Угол отклонения шеста   | ≈ -0.418 рад (−24°) | ≈ 0.418 рад (+24°) |\n",
        "| 3  | Угловая скорость шеста  | -∞                 | +∞                 |\n",
        "\n",
        "**Примечание:** приведённые выше диапазоны — это возможные значения в пространстве наблюдений, но **эпизод** закончится при выходе за более жёсткие границы:\n",
        "\n",
        "- Положение тележки (индекс 0) может лежать в `(-4.8, 4.8)`, но эпизод прерывается, если тележка уходит за `(-2.4, 2.4)`.  \n",
        "- Угол шеста может наблюдаться в `(-0.418, 0.418)` рад (±24°), но эпизод прерывается, если угол выходит за `(-0.2095, 0.2095)` рад (±12°).\n",
        "\n",
        "---\n",
        "\n",
        "### Награды\n",
        "\n",
        "За каждый шаг (включая последний) начисляется **+1** балл. Максимальная длина эпизода — **500** шагов для версии **v1** и **200** шагов для версии **v0**.\n",
        "\n",
        "---\n",
        "\n",
        "### Начальное состояние\n",
        "\n",
        "В начале эпизода все четыре компоненты наблюдения равномерно случайны в диапазоне **(-0.05, 0.05)**.\n",
        "\n",
        "---\n",
        "\n",
        "### Завершение эпизода\n",
        "\n",
        "Эпизод заканчивается при выполнении любого из условий:\n",
        "\n",
        "1. **Терминация:** угол отклонения шеста превышает ±12°.  \n",
        "2. **Терминация:** положение тележки выходит за ±2.4 (центр тележки достигает края экрана).  \n",
        "3. **Ограничение по длине:** число шагов превышает 500 (200 для **v0**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9d8fea0d-7e29-4eea-888c-26e2bf9f16c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d8fea0d-7e29-4eea-888c-26e2bf9f16c4",
        "outputId": "e703e03a-206b-4489-8c73-ff0e9e576ebd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env.observation_space=Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "env.action_space=Discrete(2)\n",
            "Action_space: 2 | State_space: (4,)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", max_episode_steps=1000)\n",
        "env.reset()\n",
        "\n",
        "# Выведем информацию о пространствах состояний и действий\n",
        "print(f'{env.observation_space=}')\n",
        "print(f'{env.action_space=}')\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "print(f'Action_space: {n_actions} | State_space: {state_dim}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381",
      "metadata": {
        "id": "febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
        "\n",
        "Будем использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``). Сигмоиды и другие похожие функции активации могут плохо работать с ненормализованными входными данными.\n",
        "\n",
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "$$\n",
        "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]\n",
        "$$\n",
        "$$\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "где\n",
        "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n",
        "* $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В научных статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$. В PyTorch есть метод `.detach()` класса `Tensor`, который возвращает тензор с выключенными градиентами, а также контекстный менеджер `with torch.no_grad()`, который задает контекст с вычислениями, для которых не вычисляется градиент."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f22be5c6-da87-46aa-95ec-ec792e46a355",
      "metadata": {
        "id": "f22be5c6-da87-46aa-95ec-ec792e46a355"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def create_network(input_dim, hidden_dims, output_dim):\n",
        "    layers: list[nn.Module] = []\n",
        "    prev_dim = input_dim\n",
        "\n",
        "    for h in hidden_dims:\n",
        "        layers.append(nn.Linear(prev_dim, h))\n",
        "        layers.append(nn.ReLU())\n",
        "        prev_dim = h\n",
        "\n",
        "    layers.append(nn.Linear(prev_dim, output_dim))\n",
        "\n",
        "    network = nn.Sequential(*layers)\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25af58f1-46d1-4cf4-abf7-e6f62161595d",
      "metadata": {
        "id": "25af58f1-46d1-4cf4-abf7-e6f62161595d"
      },
      "source": [
        "Добавим $\\epsilon$-жадный выбор действий:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d1684ea0-11e4-4f16-a7d3-45c97dbcbc99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1684ea0-11e4-4f16-a7d3-45c97dbcbc99",
        "outputId": "f85c26be-ea5a-4290-d5c3-6b28567ad48e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def select_action_eps_greedy(Q, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = Q(state).detach().numpy()\n",
        "\n",
        "    if np.random.rand() < epsilon:\n",
        "        action = np.random.randint(Q_s.shape[0])\n",
        "    else:\n",
        "        action = np.argmax(Q_s)\n",
        "\n",
        "    action = int(action)\n",
        "    return action\n",
        "\n",
        "\n",
        "Q = create_network(\n",
        "    input_dim=np.prod(state_dim), hidden_dims=[64, 64], output_dim=n_actions\n",
        ")\n",
        "select_action_eps_greedy(Q, env.reset()[0].flatten(), epsilon=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef",
      "metadata": {
        "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef"
      },
      "outputs": [],
      "source": [
        "def to_tensor(x, dtype=np.float32):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    x = np.asarray(x, dtype=dtype)\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "\n",
        "def compute_td_target(\n",
        "        Q, rewards, next_states, terminateds, gamma=0.99, check_shapes=True,\n",
        "):\n",
        "    \"\"\" Считает TD-target.\"\"\"\n",
        "\n",
        "    r = to_tensor(rewards)\n",
        "    s_next = to_tensor(next_states)\n",
        "    term = to_tensor(terminateds, bool)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        Q_sn = Q(s_next)\n",
        "        V_sn = torch.max(Q_sn, dim=1)[0]\n",
        "\n",
        "    target = r + gamma * V_sn * (~term)\n",
        "\n",
        "\n",
        "    assert V_sn.dtype == torch.float32\n",
        "\n",
        "    if check_shapes:\n",
        "        assert Q_sn.data.dim() == 2, \\\n",
        "            \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert V_sn.data.dim() == 1, \\\n",
        "            \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target.data.dim() == 1, \\\n",
        "            \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return target\n",
        "\n",
        "\n",
        "def compute_td_loss(\n",
        "        Q, states, actions, td_target, regularizer=.1, out_non_reduced_losses=False\n",
        "):\n",
        "    \"\"\" Считает TD ошибку.\"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    s = to_tensor(states)  # shape: [batch_size, state_size]\n",
        "    a = to_tensor(actions, int).long()  # shape: [batch_size]\n",
        "    target = to_tensor(td_target)\n",
        "\n",
        "    Q_s = Q(s)\n",
        "    Q_s_a = Q_s.gather(dim=1, index=a.unsqueeze(1)).squeeze(1)\n",
        "    td_error = Q_s_a - target\n",
        "\n",
        "\n",
        "    td_losses = td_error ** 2\n",
        "    loss = torch.mean(td_losses)\n",
        "    # добавляем L1 регуляризацию на значения Q\n",
        "    loss += regularizer * torch.abs(Q_s_a).mean()\n",
        "\n",
        "    if out_non_reduced_losses:\n",
        "        return loss, td_losses.detach()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d",
      "metadata": {
        "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d"
      },
      "outputs": [],
      "source": [
        "def eval_dqn(env_name, Q):\n",
        "    \"\"\"Оценка качества работы алгоритма на одном эпизоде\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    s, _ = env.reset()\n",
        "    done, ep_return = False, 0.\n",
        "\n",
        "    while not done:\n",
        "        # set epsilon = 0 to make an agent act greedy\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=0.)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "        ep_return += r\n",
        "        s = s_next\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return ep_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "d2be9060-bba6-4d9a-9e4b-0aa50f88839e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2be9060-bba6-4d9a-9e4b-0aa50f88839e",
        "outputId": "ddde0c05-353e-4999-b525-a3f62bbeb3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global_step=250 | avg_return=10.000 | epsilon=0.396\n",
            "global_step=500 | avg_return=9.500 | epsilon=0.392\n",
            "global_step=750 | avg_return=15.667 | epsilon=0.389\n",
            "global_step=1000 | avg_return=15.500 | epsilon=0.385\n",
            "global_step=1250 | avg_return=15.800 | epsilon=0.381\n",
            "global_step=1500 | avg_return=16.000 | epsilon=0.377\n",
            "global_step=1750 | avg_return=15.429 | epsilon=0.373\n",
            "global_step=2000 | avg_return=19.750 | epsilon=0.370\n",
            "global_step=2250 | avg_return=19.333 | epsilon=0.366\n",
            "global_step=2500 | avg_return=20.600 | epsilon=0.362\n",
            "global_step=2750 | avg_return=22.400 | epsilon=0.358\n",
            "global_step=3000 | avg_return=26.900 | epsilon=0.354\n",
            "global_step=3250 | avg_return=30.300 | epsilon=0.351\n",
            "global_step=3500 | avg_return=30.800 | epsilon=0.347\n",
            "global_step=3750 | avg_return=32.400 | epsilon=0.343\n",
            "global_step=4000 | avg_return=42.200 | epsilon=0.339\n",
            "global_step=4250 | avg_return=46.100 | epsilon=0.335\n",
            "global_step=4500 | avg_return=44.300 | epsilon=0.332\n",
            "global_step=4750 | avg_return=45.900 | epsilon=0.328\n",
            "global_step=5000 | avg_return=47.700 | epsilon=0.324\n",
            "global_step=5250 | avg_return=48.800 | epsilon=0.320\n",
            "global_step=5500 | avg_return=46.400 | epsilon=0.316\n",
            "global_step=5750 | avg_return=42.900 | epsilon=0.313\n",
            "global_step=6000 | avg_return=45.600 | epsilon=0.309\n",
            "global_step=6250 | avg_return=46.200 | epsilon=0.305\n",
            "global_step=6500 | avg_return=46.000 | epsilon=0.301\n",
            "global_step=6750 | avg_return=44.300 | epsilon=0.297\n",
            "global_step=7000 | avg_return=47.000 | epsilon=0.294\n",
            "global_step=7250 | avg_return=51.600 | epsilon=0.290\n",
            "global_step=7500 | avg_return=52.400 | epsilon=0.286\n",
            "global_step=7750 | avg_return=50.200 | epsilon=0.282\n",
            "global_step=8000 | avg_return=51.700 | epsilon=0.278\n",
            "global_step=8250 | avg_return=56.100 | epsilon=0.275\n",
            "global_step=8500 | avg_return=54.400 | epsilon=0.271\n",
            "global_step=8750 | avg_return=65.700 | epsilon=0.267\n",
            "global_step=9000 | avg_return=69.700 | epsilon=0.263\n",
            "global_step=9250 | avg_return=79.300 | epsilon=0.259\n",
            "global_step=9500 | avg_return=81.200 | epsilon=0.256\n",
            "global_step=9750 | avg_return=93.400 | epsilon=0.252\n",
            "global_step=10000 | avg_return=107.000 | epsilon=0.248\n",
            "global_step=10250 | avg_return=122.100 | epsilon=0.244\n",
            "global_step=10500 | avg_return=131.700 | epsilon=0.240\n",
            "global_step=10750 | avg_return=138.100 | epsilon=0.237\n",
            "global_step=11000 | avg_return=150.200 | epsilon=0.233\n",
            "global_step=11250 | avg_return=152.500 | epsilon=0.229\n",
            "global_step=11500 | avg_return=158.000 | epsilon=0.225\n",
            "global_step=11750 | avg_return=163.500 | epsilon=0.221\n",
            "global_step=12000 | avg_return=166.000 | epsilon=0.218\n",
            "global_step=12250 | avg_return=148.300 | epsilon=0.214\n",
            "global_step=12500 | avg_return=143.000 | epsilon=0.210\n",
            "global_step=12750 | avg_return=154.800 | epsilon=0.206\n",
            "global_step=13000 | avg_return=160.700 | epsilon=0.202\n",
            "global_step=13250 | avg_return=176.600 | epsilon=0.199\n",
            "global_step=13500 | avg_return=192.500 | epsilon=0.195\n",
            "global_step=13750 | avg_return=225.000 | epsilon=0.191\n",
            "Решено!\n"
          ]
        }
      ],
      "source": [
        "from collections import deque\n",
        "\n",
        "def linear(st, end, duration, t):\n",
        "    \"\"\"\n",
        "    Линейная интерполяция значений в пределах диапазона [st, end],\n",
        "    используя прогресс по времени t относительно всего отведенного\n",
        "    времени duration.\n",
        "    \"\"\"\n",
        "\n",
        "    if t >= duration:\n",
        "        return end\n",
        "    return st + (end - st) * (t / duration)\n",
        "\n",
        "def run_dqn(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(128, 128), lr=1e-3, gamma=0.99,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n",
        "        train_schedule=1, eval_schedule=1000, smooth_ret_window=10, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps + 1):\n",
        "        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n",
        "\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            opt.zero_grad()\n",
        "            td_target = compute_td_target(Q, [r], [s_next], [terminated], gamma=gamma)\n",
        "            loss = compute_td_loss(Q, [s], [a], td_target)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn(eval_schedule=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6aa1f1e-f346-4081-9aa9-4a1f6655152c",
      "metadata": {
        "id": "b6aa1f1e-f346-4081-9aa9-4a1f6655152c"
      },
      "source": [
        "Комментарии к получаемым результатам:\n",
        "- `avg_return` - это средняя отдача за эпизод на истории из последних десяти эпизодов. В случае корректной реализации, этот показатель будет низким первые 1000 шагов и только затем будет возрастать и сойдется на 5000-15000 шагах в зависимости от архитектуры сети.\n",
        "- Если сеть не достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте начальный $\\epsilon$.\n",
        "- Переменная `epsilon` обеспечивает стремление агента исследовать среду. В данной реализации используется линейное затухание для частоты исследования."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be1c2a8-37c6-413d-9d57-eff7251943ab",
      "metadata": {
        "id": "6be1c2a8-37c6-413d-9d57-eff7251943ab"
      },
      "source": [
        "### DQN with Experience Replay\n",
        "\n",
        "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', 1_\\text{terminated})\\}$.\n",
        "\n",
        "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "a7b5af1a-cd63-4642-9ec2-f1dc71215dff",
      "metadata": {
        "id": "a7b5af1a-cd63-4642-9ec2-f1dc71215dff"
      },
      "outputs": [],
      "source": [
        "def sample_batch(replay_buffer, n_samples):\n",
        "    rng = np.random.default_rng()\n",
        "    buffer_size = len(replay_buffer)\n",
        "    replace = buffer_size < n_samples\n",
        "    indices = rng.choice(buffer_size, size=n_samples, replace=replace)\n",
        "    batch = [replay_buffer[i] for i in indices]\n",
        "    states, actions, rewards, next_states, terminateds = zip(*batch)\n",
        "\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(terminateds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "76a8db42-6e80-4a38-8449-f13c50ef849c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76a8db42-6e80-4a38-8449-f13c50ef849c",
        "outputId": "55a16f7c-66b6-4f57-de62-5353a0e38533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global_step=250 | avg_return=10.000 | epsilon=0.396\n",
            "global_step=500 | avg_return=9.500 | epsilon=0.392\n",
            "global_step=750 | avg_return=16.667 | epsilon=0.389\n",
            "global_step=1000 | avg_return=22.250 | epsilon=0.385\n",
            "global_step=1250 | avg_return=24.000 | epsilon=0.381\n",
            "global_step=1500 | avg_return=28.000 | epsilon=0.377\n",
            "global_step=1750 | avg_return=35.400 | epsilon=0.373\n",
            "global_step=2000 | avg_return=45.400 | epsilon=0.370\n",
            "global_step=2250 | avg_return=61.000 | epsilon=0.366\n",
            "global_step=2500 | avg_return=77.400 | epsilon=0.362\n",
            "global_step=2750 | avg_return=84.000 | epsilon=0.358\n",
            "global_step=3000 | avg_return=106.200 | epsilon=0.354\n",
            "global_step=3250 | avg_return=110.800 | epsilon=0.351\n",
            "global_step=3500 | avg_return=152.200 | epsilon=0.347\n",
            "global_step=3750 | avg_return=148.000 | epsilon=0.343\n",
            "global_step=4000 | avg_return=190.400 | epsilon=0.339\n",
            "global_step=4250 | avg_return=194.200 | epsilon=0.335\n",
            "global_step=4500 | avg_return=207.800 | epsilon=0.332\n",
            "Решено!\n"
          ]
        }
      ],
      "source": [
        "def run_dqn_rb(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(256, 256), lr=1e-3, gamma=0.99,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n",
        "        train_schedule=4, replay_buffer_size=400, batch_size=32,\n",
        "        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps + 1):\n",
        "        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        replay_buffer.append((s, a, r, s_next, terminated))\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            train_batch = sample_batch(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, terminateds = train_batch\n",
        "\n",
        "            opt.zero_grad()\n",
        "            td_target = compute_td_target(Q, rewards, next_states, terminateds, gamma=gamma)\n",
        "            loss = compute_td_loss(Q, states, actions, td_target)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn_rb(eval_schedule=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "271c3132-f2eb-4d5a-8ba4-8811421ce3cd",
      "metadata": {
        "id": "271c3132-f2eb-4d5a-8ba4-8811421ce3cd"
      },
      "source": [
        "## DQN with Prioritized Experience Replay\n",
        "\n",
        "Добавим каждому примеру, хранящемуся в памяти, значение приоритета. Приоритет будет влиять на частоту случайного выбора примеров в пакет на обучение. Удачный выбор приоритета позволит повысить эффективность обучения. Популярным вариантом является абсолютное значение TD-ошибки. Таким образом акцент при обучении Q-функции отводится примерам, на которых аппроксиматор ошибается сильнее.\n",
        "\n",
        "Однако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз накладно. Из-за этого потребуется искать баланс между точностью оценки приоритета и скоростью работы.\n",
        "\n",
        "В данном задании мы будем делать следующее:\n",
        "\n",
        "- Использовать TD-ошибку в качестве приоритета.\n",
        "- Так как для пакета данных, используемых при обучении, в любом случае будет вычислена TD-ошибка, воспользуемся полученными значениями для обновления значений приоритета в памяти для каждого примера из данного пакета.\n",
        "- Будем периодически сортировать память для того, чтобы новые добавляемые переходы заменяли собой те переходы, у которых наименьший приоритет (т.е. наименьшие значения ошибки). Сортировка - дорогостоящая операция, поэтому выбрана редкая периодичность.\n",
        "\n",
        "NB: Обратите внимание, что софтмакс очень чувствителен к масштабу величин и часто требует подбора температуры. Чтобы частично нивелировать эту проблему, предлагается использовать не `softmax(priorities)` напрямую, а воспользоваться функцией $\\text{symlog} = \\text{sign}(x) \\cdot \\log (|x| + 1)$, то есть `softmax(symlog(priorities))`, и не подбирать температуру. Идея взята из статьи DreamerV2 —-- в этой статье можно ознакомиться с идеей применения функций *symlog* и *simexp*, так как это полезная альтернатива нормализации некоторых величин в RL (вознаграждений, отдач, полезностей, логитов)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3ae2d18b-bb97-4a8f-94ee-259935c039b9",
      "metadata": {
        "id": "3ae2d18b-bb97-4a8f-94ee-259935c039b9"
      },
      "outputs": [],
      "source": [
        "def symlog(x):\n",
        "    \"\"\"\n",
        "    Compute symlog values for a vector `x`.\n",
        "    It's an inverse operation for symexp.\n",
        "    \"\"\"\n",
        "    return np.sign(x) * np.log(np.abs(x) + 1)\n",
        "\n",
        "def softmax(xs, temp=1.):\n",
        "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "def sample_prioritized_batch(replay_buffer, n_samples):\n",
        "    if len(replay_buffer) < n_samples:\n",
        "        n_samples = len(replay_buffer)\n",
        "    \n",
        "    buffer_size = len(replay_buffer)\n",
        "    replace = buffer_size < n_samples\n",
        "    priorities = np.array([sample[0] for sample in replay_buffer], dtype=np.float32)\n",
        "    scores = symlog(priorities)\n",
        "    probs = softmax(scores)\n",
        "    indices = np.random.choice(buffer_size, size=n_samples, replace=replace, p=probs)\n",
        "\n",
        "    batch_samples = [replay_buffer[i] for i in indices]\n",
        "    states, actions, rewards, next_states, terminateds = zip(\n",
        "        *[(s, a, r, sn, t) for (_, s, a, r, sn, t) in batch_samples]\n",
        "    )\n",
        "\n",
        "    batch = (\n",
        "        np.array(states), np.array(actions), np.array(rewards),\n",
        "        np.array(next_states), np.array(terminateds)\n",
        "    )\n",
        "    return batch, indices\n",
        "\n",
        "def update_batch(replay_buffer, indices, batch, new_priority):\n",
        "    \"\"\"Updates batches with corresponding indices\n",
        "    replacing their priority values.\"\"\"\n",
        "    states, actions, rewards, next_states, terminateds = batch\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "        new_batch = (\n",
        "            new_priority[i], states[i], actions[i], rewards[i],\n",
        "            next_states[i], terminateds[i]\n",
        "        )\n",
        "        replay_buffer[indices[i]] = new_batch\n",
        "\n",
        "def sort_replay_buffer(replay_buffer):\n",
        "    \"\"\"Sorts replay buffer to move samples with\n",
        "    lesser priority to the beginning ==> they will be\n",
        "    replaced with the new samples sooner.\"\"\"\n",
        "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
        "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
        "    return new_rb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "88dcccfe-159d-48b3-adcb-d1d5459cdf84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88dcccfe-159d-48b3-adcb-d1d5459cdf84",
        "outputId": "34cab5b9-1adc-4eb1-e849-62f634b08f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "global_step=250 | avg_return=9.000 | epsilon=0.396\n",
            "global_step=500 | avg_return=9.000 | epsilon=0.392\n",
            "global_step=750 | avg_return=14.333 | epsilon=0.389\n",
            "global_step=1000 | avg_return=49.000 | epsilon=0.385\n",
            "global_step=1250 | avg_return=70.400 | epsilon=0.381\n",
            "global_step=1500 | avg_return=89.800 | epsilon=0.377\n",
            "global_step=1750 | avg_return=103.400 | epsilon=0.373\n",
            "global_step=2000 | avg_return=100.000 | epsilon=0.370\n",
            "global_step=2250 | avg_return=90.800 | epsilon=0.366\n",
            "global_step=2500 | avg_return=84.800 | epsilon=0.362\n",
            "global_step=2750 | avg_return=98.400 | epsilon=0.358\n",
            "global_step=3000 | avg_return=116.200 | epsilon=0.354\n",
            "global_step=3250 | avg_return=151.800 | epsilon=0.351\n",
            "global_step=3500 | avg_return=172.800 | epsilon=0.347\n",
            "global_step=3750 | avg_return=199.800 | epsilon=0.343\n",
            "global_step=4000 | avg_return=221.200 | epsilon=0.339\n",
            "Решено!\n"
          ]
        }
      ],
      "source": [
        "def run_dqn_prioritized_rb(\n",
        "        env_name=\"CartPole-v1\",\n",
        "        hidden_dims=(256, 256), lr=1e-3, gamma=0.99,\n",
        "        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n",
        "        train_schedule=4, replay_buffer_size=400, batch_size=32,\n",
        "        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n",
        "):\n",
        "    env = gym.make(env_name)\n",
        "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
        "\n",
        "    Q = create_network(\n",
        "        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims,\n",
        "        output_dim=env.action_space.n\n",
        "    )\n",
        "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
        "\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for global_step in range(1, total_max_steps + 1):\n",
        "        epsilon = linear(\n",
        "            eps_st, eps_end, eps_dur * total_max_steps, global_step\n",
        "        )\n",
        "        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            s_t = to_tensor(s).unsqueeze(0)\n",
        "            s_next_t = to_tensor(s_next).unsqueeze(0)\n",
        "\n",
        "            Q_val = Q(s_t)                     # [1, n_actions]\n",
        "            Q_next = Q(s_next_t)               # [1, n_actions]\n",
        "            V_next, _ = Q_next.max(dim=1)      # [1]\n",
        "\n",
        "            mask = 0.0 if terminated else 1.0\n",
        "            td_target_single = torch.tensor(r, dtype=torch.float32) + gamma * V_next[0] * mask\n",
        "            Q_sa = Q_val[0, a]\n",
        "\n",
        "            # абсолютная TD-ошибка\n",
        "            loss = (Q_sa - td_target_single).abs().item()\n",
        "\n",
        "        replay_buffer.append((loss, s, a, r, s_next, terminated))\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if global_step % train_schedule == 0:\n",
        "            train_batch, indices = sample_prioritized_batch(\n",
        "                replay_buffer, batch_size\n",
        "            )\n",
        "            (\n",
        "                states, actions, rewards,\n",
        "                next_states, terminateds\n",
        "            ) = train_batch\n",
        "\n",
        "            opt.zero_grad()\n",
        "            td_target = compute_td_target(Q, rewards, next_states, terminateds, gamma=gamma)\n",
        "            loss, td_losses = compute_td_loss(Q, states, actions, td_target, out_non_reduced_losses=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            update_batch(\n",
        "                replay_buffer, indices, train_batch, td_losses.numpy()\n",
        "            )\n",
        "\n",
        "        # with much slower scheduler periodically re-sort replay buffer\n",
        "        # such that it will overwrite the least important samples\n",
        "        if global_step % (10 * train_schedule) == 0:\n",
        "            replay_buffer = sort_replay_buffer(replay_buffer)\n",
        "\n",
        "        if global_step % eval_schedule == 0:\n",
        "            eval_return = eval_dqn(env_name, Q)\n",
        "            eval_return_history.append(eval_return)\n",
        "            avg_return = np.mean(eval_return_history)\n",
        "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
        "            if avg_return >= success_ret:\n",
        "                print('Решено!')\n",
        "                break\n",
        "\n",
        "        s = s_next\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "run_dqn_prioritized_rb(eval_schedule=250)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "31f4ab6b-d997-4bf1-aca0-f69778510244",
        "242e0dc7-8596-4446-921e-7cb76fc72a0e",
        "1cfd75e8-6372-484b-9e8a-1121dcb9822f",
        "42cef2bf-7171-4bc4-9e03-4ca40b7d3a83",
        "90da8500-4981-4f20-bab7-299b337de4c6",
        "91e23ed3-ad53-4bf6-9586-615056aa8cd8",
        "3dc8331c-a511-4ae2-8935-6deae52c2030",
        "77b4c2f2-479f-4940-b2b0-c007d0b529dd",
        "df6fa045-56ea-4c5c-95c8-4f6efe11b9e9",
        "34023df6-54de-4c28-8913-6d7c51f66be1",
        "a2c4700b-032a-4a48-abbd-218ad2cbbcbb",
        "041bf63d-b50d-4131-b8ef-f45564162593",
        "cb4c2cdc-bf4c-4ca2-8296-df55e4a1725e",
        "db1c5d6b-42ac-4a7b-bed2-7f48c067c357"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
